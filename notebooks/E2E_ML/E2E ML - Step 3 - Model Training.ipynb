{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42a2a44d-1c9b-4f7f-b05e-d6b60e0a4fa5",
   "metadata": {
    "collapsed": false,
    "name": "cell25"
   },
   "source": [
    "Before running this noetbook makes sure that the following packages are installed in your Python enviroment:\n",
    "\n",
    "* `matplotlib`\n",
    "* `seaborn`\n",
    "* `networkx`\n",
    "* `snowflake-snowpark-python`\n",
    "* `snowflake-ml-python`\n",
    "* `snowflake`\n",
    "\n",
    "You also need to make sure the file `plotting.py` is in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from snowflake.ml.feature_store import (\n",
    "    FeatureStore,\n",
    "    CreationMode)\n",
    "\n",
    "from snowflake.ml.modeling.impute import SimpleImputer\n",
    "from snowflake.ml.modeling.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from snowflake.ml.modeling.pipeline import Pipeline\n",
    "from snowflake.ml.modeling.xgboost import XGBRegressor\n",
    "from snowflake.ml.modeling.model_selection import GridSearchCV\n",
    "from snowflake.ml.modeling.metrics import mean_absolute_percentage_error\n",
    "from snowflake.ml.registry import Registry\n",
    "\n",
    "from snowflake.snowpark import Session\n",
    "import snowflake.snowpark.functions as snow_funcs\n",
    "\n",
    "from snowflake.core import Root\n",
    "from snowflake.core.warehouse import Warehouse\n",
    "\n",
    "# Import a function from a python file on a stage\n",
    "from plotting import plot_lineage\n",
    "\n",
    "# Get the Snowpark session\n",
    "from snowflake.snowpark import Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Snowflake\n",
    "\n",
    "This example is using the connections.toml file to connect to Snowflake. You can read more at https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-connect#connecting-using-the-connections-toml-file how to set it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce110000-1111-2222-3333-ffffff000002",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "db_name = \"SNOWPARK_DEMO_DB\"\n",
    "schema_name = \"SIMPLE_ML_SCHEMA\"\n",
    "fs_schema_name = \"SIMPLE_FS_SCHEMA\"\n",
    "mr_schema_name = \"SIMPLE_MR_SCHEMA\"\n",
    "wh_name = \"SIMPLE_ML_WH\"\n",
    "\n",
    "CONNECTION_NAME = '<YOUR CONNECTION NAME>' # Name of the connection in connections.toml to be used to connect to Snowflake\n",
    "\n",
    "session = Session.builder.config(\"connection_name\", CONNECTION_NAME).create()\n",
    "\n",
    "session.use_schema(f'{db_name}.{schema_name}')\n",
    "session.use_warehouse(wh_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce110000-1111-2222-3333-ffffff000003",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "# Connect to Feature Store\n",
    "fs = FeatureStore(\n",
    "    session=session, \n",
    "    database=db_name, \n",
    "    name=fs_schema_name, \n",
    "    default_warehouse=wh_name,\n",
    "    creation_mode=CreationMode.FAIL_IF_NOT_EXIST,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000004",
   "metadata": {
    "collapsed": false,
    "name": "cell5"
   },
   "source": [
    "### Generate a Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000005",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "fs.list_entities().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000006",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "fs.list_feature_views().filter(snow_funcs.array_contains(snow_funcs.to_variant(snow_funcs.lit('CUSTOMER')), snow_funcs.col(\"ENTITIES\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce110000-1111-2222-3333-ffffff000007",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "cust_fv = fs.get_feature_view(name=\"CUSTOMER_GENERAL_DATA_FEATURES\",\n",
    "                                version=\"V1\")\n",
    "behavior_fv = fs.get_feature_view(name=\"CUSTOMER_BEHAVIOR_DATA_FEATURES\",\n",
    "                                version=\"V1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a spine dataframe that has the EMAIL's of all customers we want to get features for, it also have our target column for the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000008",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "print('Customer Life Time Value Data:')\n",
    "ltv_df = session.table(f'{db_name}.{schema_name}.CUSTOMER_LIFE_TIME_VALUE').drop('YEAR_MONTH')\n",
    "ltv_df.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a Dataset with features for all customers in ltv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000009",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": [
    "registered_dataset = fs.generate_dataset(\n",
    "    name=\"ECOMMERCE_CUSTOMER_FEATURES\",\n",
    "    spine_df=ltv_df,\n",
    "    features=[cust_fv,behavior_fv],\n",
    "    spine_label_cols=[\"LIFE_TIME_VALUE\"],\n",
    "    desc=\"Training Data to train model to predict Customer Life Time Value.\"\n",
    ")\n",
    "\n",
    "\n",
    "# Retrieve a Snowpark DataFrame from the registered Dataset\n",
    "# We can also get a Pandas DataFrame (to_pandas) or a TensorFlow tf.data.Dataset or a Pytorch datapipe or a PyTorch Iterable Dataset\n",
    "registered_dataset_df = registered_dataset.read.to_snowpark_dataframe()\n",
    "registered_dataset_df.limit(5).show()\n",
    "\n",
    "#training_set = fs.generate_training_set(\n",
    "#    spine_df=ltv_df,\n",
    "#    features=[cust_fv,behavior_fv],\n",
    "#    spine_label_cols=[\"LIFE_TIME_VALUE\"],      # optional\n",
    "#)\n",
    "#training_set.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipline that impute, encode and train a XGBRegressor using GridSearchCV.\n",
    "\n",
    "The GridSearchCV will run in a distributed mode withinh Snowflake, the large the Warehouse used the more distributed it will be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce110000-1111-2222-3333-ffffff000010",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_df, test_df = registered_dataset_df.random_split(weights=[0.9, 0.1], seed=0)\n",
    "train_df.count(), test_df.count()\n",
    "\n",
    "# Drop the Email column for Training\n",
    "train_df = train_df.drop('EMAIL')\n",
    "\n",
    "# Define sklearn-like Imputers\n",
    "si_numeric =  SimpleImputer(\n",
    "    input_cols=['AVG_SESSION_LENGTH_MIN','AVG_TIME_ON_APP_MIN','AVG_TIME_ON_WEBSITE_MIN'], \n",
    "    output_cols=['AVG_SESSION_LENGTH_MIN_IMP','AVG_TIME_ON_APP_MIN_IMP','AVG_TIME_ON_WEBSITE_MIN_IMP'],\n",
    "    strategy='mean',\n",
    "    drop_input_cols=False\n",
    ")\n",
    "\n",
    "# Define sklearn-like Encoders\n",
    "categories = {\n",
    "    \"MEMBERSHIP_STATUS\": np.array([\"BASIC\", \"BRONZE\", \"SILVER\", \"GOLD\", \"PLATIN\", \"DIAMOND\"]),\n",
    "}\n",
    "oe_categorical = OrdinalEncoder(\n",
    "    input_cols=[\"MEMBERSHIP_STATUS\"], \n",
    "    output_cols=[\"MEMBERSHIP_STATUS_OE\"], \n",
    "    categories=categories,\n",
    "    drop_input_cols=False\n",
    ")\n",
    "\n",
    "ohe_categorical = OneHotEncoder(\n",
    "    input_cols=[\"GENDER\"], \n",
    "    output_cols=[\"GENDER_OHE\"],\n",
    "    drop_input_cols=False\n",
    ")\n",
    "\n",
    "# Define the XGBoost model (incl. Hyperparameter Tuning)\n",
    "feature_cols = [\n",
    "    'GENDER_OHE_FEMALE',\n",
    "    'GENDER_OHE_MALE',\n",
    "    'MEMBERSHIP_STATUS_OE',\n",
    "    'MEMBERSHIP_LENGTH_DAYS',\n",
    "    'AVG_SESSION_LENGTH_MIN_IMP',\n",
    "    'AVG_TIME_ON_APP_MIN_IMP',\n",
    "    'AVG_TIME_ON_WEBSITE_MIN_IMP',\n",
    "    'APP_PRIMARY'\n",
    "]\n",
    "label_cols = ['LIFE_TIME_VALUE']\n",
    "output_cols = ['LIFE_TIME_VALUE_PREDICTION']\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=XGBRegressor(),\n",
    "    param_grid={\n",
    "        \"n_estimators\":[100, 200, 300, 400],\n",
    "        \"learning_rate\":[0.1, 0.2, 0.3],\n",
    "    },\n",
    "    n_jobs = -1,\n",
    "    scoring=\"neg_mean_absolute_percentage_error\",\n",
    "    input_cols=feature_cols,\n",
    "    label_cols=label_cols,\n",
    "    output_cols=output_cols\n",
    ")\n",
    "\n",
    "# Build the pipeline\n",
    "model_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"SI_NUMERIC\",si_numeric),\n",
    "        (\"OE_CATEGORICAL\",oe_categorical),\n",
    "        (\"OHE_CATEGORICAL\",ohe_categorical),\n",
    "        (\"GRIDSEARCH_XGBOOST\",grid_search)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale up the Warehouse using the Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce110000-1111-2222-3333-ffffff000011",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": [
    "# Scale up our WH\n",
    "root = Root(session)\n",
    "\n",
    "ml_wh = Warehouse(\n",
    "  name=wh_name,\n",
    "  warehouse_size=\"XXLARGE\",\n",
    "  wait_for_completion = \"true\",\n",
    ")\n",
    "\n",
    "ml_wh_res = root.warehouses[wh_name]\n",
    "\n",
    "ml_wh_res.create_or_alter(ml_wh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000012",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "# Fit the pipeline to the training data\n",
    "fitted_pipeline = model_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale down the Warehouse since we no longer need it as large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce110000-1111-2222-3333-ffffff000013",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "ml_wh = Warehouse(\n",
    "  name=wh_name,\n",
    "  warehouse_size=\"SMALL\",\n",
    "  wait_for_completion = \"true\",\n",
    ")\n",
    "\n",
    "ml_wh_res = root.warehouses[wh_name]\n",
    "\n",
    "ml_wh_res.create_or_alter(ml_wh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000014",
   "metadata": {
    "collapsed": false,
    "name": "cell15"
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000015",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": [
    "# Analyze grid search results\n",
    "gs_results = fitted_pipeline.to_sklearn().named_steps['GRIDSEARCH_XGBOOST'].cv_results_\n",
    "n_estimators_val = []\n",
    "learning_rate_val = []\n",
    "for param_dict in gs_results[\"params\"]:\n",
    "    n_estimators_val.append(param_dict[\"n_estimators\"])\n",
    "    learning_rate_val.append(param_dict[\"learning_rate\"])\n",
    "mape_val = gs_results[\"mean_test_score\"]*-1\n",
    "\n",
    "gs_results_df = pd.DataFrame(data={\n",
    "    \"n_estimators\":n_estimators_val,\n",
    "    \"learning_rate\":learning_rate_val,\n",
    "    \"mape\":mape_val})\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=0.5)\n",
    "sns.relplot(data=gs_results_df, x=\"learning_rate\", y=\"mape\", hue=\"n_estimators\", kind=\"line\", height=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caluclate MEAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000016",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": [
    "# Predict LTV for customers in test data\n",
    "predictions = fitted_pipeline.predict(test_df).cache_result()\n",
    "\n",
    "# Analyze results\n",
    "mape = mean_absolute_percentage_error(df=predictions, \n",
    "                                        y_true_col_names=\"LIFE_TIME_VALUE\", \n",
    "                                        y_pred_col_names=\"LIFE_TIME_VALUE_PREDICTION\")\n",
    "\n",
    "\n",
    "# Plot actual vs predicted \n",
    "g = sns.relplot(\n",
    "    data=predictions[\"LIFE_TIME_VALUE\", \"LIFE_TIME_VALUE_PREDICTION\"].to_pandas().astype(\"float64\"), \n",
    "    x=\"LIFE_TIME_VALUE\", \n",
    "    y=\"LIFE_TIME_VALUE_PREDICTION\", \n",
    "    kind=\"scatter\",\n",
    "    height=3)\n",
    "g.ax.axline((0,0), slope=1, color=\"r\")\n",
    "\n",
    "print(f\"Mean absolute percentage error: {mape}\")\n",
    "predictions.select(\"LIFE_TIME_VALUE\", \"LIFE_TIME_VALUE_PREDICTION\").limit(15).show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log model to Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000017",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": [
    "# Create reference to model registry\n",
    "ml_reg = Registry(session=session, database_name=db_name, schema_name=mr_schema_name)\n",
    "ml_reg.show_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce110000-1111-2222-3333-ffffff000018",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Register new model version\n",
    "registered_model = ml_reg.log_model(\n",
    "    fitted_pipeline,\n",
    "    model_name=\"CUSTOMER_LTV_MODEL\",\n",
    "    version_name='MY_FIRST_MODEL_VERSION',\n",
    "    comment=\"Model trained using GridsearchCV in Snowpark to predict Customer Life Time Value\",\n",
    "    metrics={\"mean_abs_pct_err\": mape},\n",
    "    conda_dependencies=['xgboost'],\n",
    "    options={'relax_version': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000019",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell20"
   },
   "outputs": [],
   "source": [
    "ml_reg.show_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000020",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell21"
   },
   "outputs": [],
   "source": [
    "registered_model.lineage('upstream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000021",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell22"
   },
   "outputs": [],
   "source": [
    "\n",
    "df = session.lineage.trace(f\"{db_name}.{mr_schema_name}.CUSTOMER_LTV_MODEL\", \"MODEL\", object_version='MY_FIRST_MODEL_VERSION', direction='upstream', distance=3)\n",
    "plot_lineage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000023",
   "metadata": {
    "language": "python",
    "name": "cell24"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks_39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
