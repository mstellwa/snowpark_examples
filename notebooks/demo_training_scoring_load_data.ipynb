{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5729cca3-231f-44b2-a44b-69c23a841081",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Demo of Snowpark end-to-end Machine Learning\n",
    "\n",
    "\n",
    "The dataset is from https://archive-beta.ics.uci.edu/dataset/222/bank+marketing\n",
    "\n",
    "\n",
    "**Run 00_Load_demo_data.ipynb to upload the Parquet files used for this Notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c0319e-3237-437a-a19b-83c3e5411a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports \n",
    "import snowflake.snowpark as S\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import types as T\n",
    "from snowflake.snowpark import Window\n",
    "\n",
    "import joblib\n",
    "import cachetools\n",
    "import io\n",
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "# Make sure we do not get line breaks when doing show on wide dataframes\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "import sqlparse\n",
    "\n",
    "# Print the version of Snowpark we are using\n",
    "print(f\"Using Snowpark: {S.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93861747-9998-42e9-81d8-8eea613c950a",
   "metadata": {},
   "source": [
    "Helper functions for nicer printing of Snowparkd dataframe schema, SQL and to generate a correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a619ad-deb3-4494-b81b-9421da9c5f14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions for nicer printing\n",
    "def print_sql(df):\n",
    "    for query in df.queries['queries']:\n",
    "        print(sqlparse.format(query, reindent=True))\n",
    "\n",
    "def print_schema(df):\n",
    "    print(\"schema:\")\n",
    "    for col in df.schema.fields:\n",
    "        print(f\" |-- {col.name}: {col.datatype} (Nullable: {col.nullable})\")\n",
    "\n",
    "def shape(df):\n",
    "    return (df.count(), len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f41d1-4a3d-43b1-9814-b2057e0896e8",
   "metadata": {},
   "source": [
    "Connect to Snowflake\n",
    "\n",
    "This example is using a JSON file with the following structure\n",
    "```\n",
    "{\n",
    "    \"account\":\"MY SNOWFLAKE ACCOUNT\",\n",
    "    \"user\": \"MY USER\",\n",
    "    \"password\":\"MY PASSWORD\",\n",
    "    \"role\":\"MY ROLE\",\n",
    "    \"warehouse\":\"MY WH\",\n",
    "    \"database\":\"MY DB\",\n",
    "    \"schema\":\"MY SCHEMA\"\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68234ce-6537-4f9e-93cd-66f29e41fc1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('../creds.json') as f:\n",
    "    connection_parameters = json.load(f)\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "print(\"Current role: \" + session.get_current_role() + \", Current schema: \" + session.get_fully_qualified_current_schema() + \", Current WH: \" + session.get_current_warehouse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aee10a-c883-4bce-8a9d-cefc0d3b991a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "source_path = \"@SOURCE_FILES/BANK_MARKETING\" # Where the source parquet files are stored\n",
    "sp_udf_stage = \"BANK_STAGE\" # Name of the stage to used for storing the code for the SP and UDF , as well the trained model files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0bbd48-5d09-47f0-850e-af485990405a",
   "metadata": {},
   "source": [
    "## Loading of source data\n",
    "### Loading Parquet files with inferring the schema.\n",
    "\n",
    "Start to check that the source files is on the stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048afc37",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.sql(f\"ls {source_path}\").select('\"name\"').show(30, max_width=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4f167e-66c5-4df1-8b70-ab5acd4055da",
   "metadata": {},
   "source": [
    "Take a peak in the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562355d5-fba0-403f-98af-c2ca90eb9e78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.sql(\"create or replace temp file format parq1 type='PARQUET'\").collect()\n",
    "session.sql(f\"select $1 from {source_path} (file_format=>parq1 )\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a45358f",
   "metadata": {},
   "source": [
    "Loading Parquet files with inferring the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2492761e-5f90-4e27-93be-a936923d2b2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_reader = session.read.parquet(source_path)\n",
    "df_reader.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c3eb39-1229-48be-bb56-2ad9a157a733",
   "metadata": {},
   "source": [
    "The df_reader datafarme is reading the files from stage when used, we can check this by looking at the SQL it generates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40494bbf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_sql(df_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7bb8ba-5e2d-4367-8ae5-c4a0d4128291",
   "metadata": {},
   "source": [
    "To load the data into a Snowflake table we can use copy_into_table.\n",
    "\n",
    "It will create the table if it not exists, using the infered schema, and if the table exists it will append the data. However, Snowflake keeps track of what files it has loaded so it does not load the same file twice, by dropping the table we ensure that the files are loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce3aaf-a806-4044-a91b-04646a3c90db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.sql(\"DROP TABLE IF EXISTS bank_marketing_v2\").collect()\n",
    "df_reader.copy_into_table(\"bank_marketing_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da084808",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "Create a Snowpark Dataframe using the new table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd2db2-4830-426a-9a8a-0e1d9c224182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_bank_marketing = session.table(\"bank_marketing_v2\")\n",
    "display(f\"Dataframe shape: {shape(df_bank_marketing)}\")\n",
    "df_bank_marketing.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff1a883-c4e9-4fb1-b9dc-bcb991a6b7eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_sql(df_bank_marketing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c08dea6-a8ea-4227-a2b2-7250ada628db",
   "metadata": {},
   "source": [
    "### Data understanding\n",
    "\n",
    "Start with verifying datatypes, simple put we will treat charcter columns as categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87a9434-7719-47f3-8fa6-d37f170c6633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_schema(df_bank_marketing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cf2667-d23c-46fd-9d83-4771f1786f79",
   "metadata": {},
   "source": [
    "DAY is stored as a number but can be threaded as categorical, fixed number of days in months, and by changing the data type to character we will do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3c8b8b-19b3-4087-a3fd-b8281b94c55f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_bank_marketing_prep = df_bank_marketing.with_column(\"DAY\", F.to_varchar(F.col(\"DAY\")))\n",
    "print_schema(df_bank_marketing_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1081a8dd-58bf-41df-b1ea-b59c4780b4e6",
   "metadata": {},
   "source": [
    "Get basic statistics about the categorical and numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5151d57-da39-4399-8fe0-20293f3d4c75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_bank_marketing_prep.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea09b2d9-d565-42c7-ab97-594ecc1e85be",
   "metadata": {},
   "source": [
    "Create variables with our categorical, numeric and target columns names so we can use them with encoders and scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b1e1e-1de7-4b80-9cc3-a10b406d0bfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_cols = [c.name for c in df_bank_marketing_prep.schema.fields if (type(c.datatype) == T.StringType) & (c.name != 'Y')]\n",
    "numeric_types = [T.DecimalType, T.LongType, T.DoubleType, T.FloatType, T.IntegerType]\n",
    "num_cols = [c.name for c in df_bank_marketing_prep.schema.fields if type(c.datatype) in numeric_types]\n",
    "target_col = \"Y\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc93ff-948e-47b7-bb5e-a665fcdf0630",
   "metadata": {},
   "source": [
    "Distribution of target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc31c3d-f546-41f4-a5e3-3b5f3973f1b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_bank_marketing_prep.group_by(target_col).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50fc992-84ed-4d2f-8f51-582163d3db3e",
   "metadata": {},
   "source": [
    "Frequency tables for each categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52afbf1e-b58f-4e27-93e2-1479baefec78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    display(df_bank_marketing_prep.select(F.count_distinct(col).as_(f\"{col} distinct values\")).show())\n",
    "    display(df_bank_marketing_prep.group_by(col).count()\\\n",
    "                                .select(col, (F.call_function(\"RATIO_TO_REPORT\", F.col(\"COUNT\")).over() * 100).as_(\"% observations\") )\\\n",
    "                                .sort(F.col(\"% observations\").desc()).show(31))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844b7a90-a815-4eb9-9015-e88b951071e5",
   "metadata": {},
   "source": [
    "Relationship between each of the categorical features and the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13778858-57b7-4ecd-99b8-10c97877c1d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    window = Window.partition_by(col)\n",
    "    display(df_bank_marketing_prep.group_by(col, F.col(target_col))\\\n",
    "                                .count()\\\n",
    "                                .select(col, F.col(target_col), (F.call_function(\"RATIO_TO_REPORT\", F.col(\"COUNT\")).over(window) * 100).as_(\"percentage\"))\\\n",
    "                                .pivot(target_col, ['no', 'yes']).agg(F.sum(\"percentage\")).show(50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817e34db-9570-4020-8d9d-5ed2364b035f",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579e01dd-2b0a-4daf-80e0-4828d57e591c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_response_model(X, y, cat_cols, num_cols):\n",
    "    \n",
    "    # One Hot Encoder transformer for categorical columns\n",
    "    cat_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    # Standard scaler for numerical columns\n",
    "    num_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Combine into a column transformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "      [\n",
    "            ('num', num_transformer, num_cols),\n",
    "            ('cat', cat_transformer, cat_cols),\n",
    "        ],  verbose_feature_names_out=False,\n",
    "    )\n",
    "    \n",
    "    # Create a pipeline with the column transformer and training of a Random Forrest Classifier\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', RandomForestClassifier(n_jobs=-1))])\n",
    "    rfc_model = pipe.fit(X, y)\n",
    "    return rfc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abf9af9-a64e-46dd-9955-6156f131bca4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd_train = df_bank_marketing.sample(frac=0.10).to_pandas()\n",
    "\n",
    "X = pd_train[[*cat_cols, *num_cols]]\n",
    "y = pd_train[\"Y\"]\n",
    "model = train_response_model(X, y, cat_cols, num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a639608-98bf-42e1-bfc9-8337bab58266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fb3033-aadc-4856-a5a7-513d7544b0ff",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9aca72-729c-465b-976b-b3833badbb03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_file_to_stage(snf_session, obj, file_name, stage_path):\n",
    "    import io\n",
    "    import joblib\n",
    "    \n",
    "    file_path = stage_path + file_name\n",
    "    \n",
    "    input_stream = io.BytesIO()\n",
    "    joblib.dump(obj, input_stream)\n",
    "    snf_session._conn._cursor.upload_stream(input_stream, file_path)\n",
    "    \n",
    "    return file_path\n",
    "\n",
    "def train_bank(snf_session: Session, stage: str) -> dict:\n",
    "    from datetime import datetime\n",
    "    \n",
    "    df_train, df_test = snf_session.table(\"bank_marketing_v2\").random_split([0.8, 0.2])\n",
    "    \n",
    "    # Get the categorical, numerical and target column\n",
    "    cat_cols = [c.name for c in df_train.schema.fields if (type(c.datatype) == T.StringType) & (c.name != 'Y')]\n",
    "    numeric_types = [T.DecimalType, T.LongType, T.DoubleType, T.FloatType, T.IntegerType]\n",
    "    num_cols = [c.name for c in df_train.schema.fields if type(c.datatype) in numeric_types]\n",
    "    target_col = \"Y\"\n",
    "    X_cols = [*cat_cols, *num_cols]\n",
    "    \n",
    "    pd_train = df_train.to_pandas()\n",
    "    X = pd_train[X_cols]\n",
    "    y = pd_train[target_col]\n",
    "    \n",
    "    # fit the pipeline\n",
    "    model = train_response_model(X, y, cat_cols, num_cols)\n",
    "    \n",
    "    # Test the model\n",
    "    pd_test = df_test.to_pandas()\n",
    "    \n",
    "    X_test = pd_test[X_cols]\n",
    "    y_test = pd_test[target_col]\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Create a dict with some test scores based on test data to return\n",
    "    ret_dict = {\"f1_score\" : f1_score(y_test, y_pred, average='macro')\n",
    "                  , \"precision_score\": precision_score(y_test, y_pred, average='macro')\n",
    "                  , \"recall_score\": recall_score(y_test, y_pred, average='macro')\n",
    "                  , \"accuracy_score\" : accuracy_score(y_test, y_pred)}\n",
    "\n",
    "    now = datetime.now()\n",
    "    # Save the model to stage\n",
    "    save_path = now.strftime(\"%Y-%m-%d-%H%M%S\")\n",
    "    model_path = save_file_to_stage(snf_session, model, 'rfc_bank_model.joblib', f'@{stage}/{save_path}/')\n",
    "    ret_dict['model_path'] = model_path\n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f69db4c-289e-4cec-b0b9-aeaa33e49ec3",
   "metadata": {},
   "source": [
    "Create a stage for storing the code for the SP, UDF and model file.\n",
    "\n",
    "If you do not want to remove the old models, skip this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de7081d-8294-4350-8548-9f69b9eaedc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.sql(f\"CREATE OR REPLACE STAGE {sp_udf_stage}\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006f5de1-350e-4083-8d81-1b56fc414f2e",
   "metadata": {},
   "source": [
    "Create the Store Procedure in Snowflake.\n",
    "The **sproc** function returns a callable object that can be used to call the stored procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634c709f-eae2-4a99-a9d6-5c66f0b901fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a88612-2c7a-41df-9c5a-475c5ef8f3b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.clear_imports()\n",
    "session.clear_packages()\n",
    "session.add_packages('snowflake-snowpark-python','pandas', 'scikit-learn==1.2.2', 'joblib')\n",
    "train_bank_sp = F.sproc(func=train_bank,name=\"train_bank\", is_permanent = True, replace= True, stage_location = f'{sp_udf_stage}/bank/sp/', session=session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e8216c-abc5-405f-934c-9d69c67a506b",
   "metadata": {},
   "source": [
    "Run the training Stored Procedure in Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e39bd-433e-4783-b0a1-9c80a80f0411",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sp_dict = json.loads(train_bank_sp(session, f'{sp_udf_stage}/bank/model'))\n",
    "sp_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db33a2c-020b-4f03-be67-06f3ef1e9c28",
   "metadata": {},
   "source": [
    "Check that the model is saved to the Snowflake stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e63d3e8-8ad6-4e3f-b6af-727b7acde73f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.sql(f\"ls @{sp_udf_stage}/bank/model\").show(max_width=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb9160b-6b45-4e0d-b0b6-ebfead947a7c",
   "metadata": {},
   "source": [
    "Deploy model as a UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3781a1-d713-48c0-a9fa-52a99e5052b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def deploy_model(snf_session, udf_name, udf_stage, model_name, import_model_path, features):\n",
    "    \n",
    "    # Function to load the model file, using cachetools makes sure file is only loaded once\n",
    "    @cachetools.cached(cache={})\n",
    "    def read_file(filename):\n",
    "        import joblib\n",
    "        import sys\n",
    "        import os\n",
    "\n",
    "        import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n",
    "        if import_dir:\n",
    "            with open(os.path.join(import_dir, filename), 'rb') as file:\n",
    "                m = joblib.load(file)\n",
    "                return m\n",
    "    \n",
    "    # Use a vectorized udf, gets maximum 100 rows at the time\n",
    "    @F.udf(name = udf_name, max_batch_size=100, is_permanent = True, stage_location = udf_stage, imports = [import_model_path]\n",
    "           , packages = ['pandas', 'scikit-learn==1.2.2', 'cachetools'], replace = True, session = snf_session)\n",
    "    def predict_response(pd_input: T.PandasDataFrame[str, str, str, str, str, str, str, str, str, str, int, int, int, int, int, int]) -> T.PandasSeries[str]:\n",
    "        # Make sure we have the columns in the expected order in the Pandas Dataframe\n",
    "        model = read_file(model_name)\n",
    "        pd_input.columns = features\n",
    "        prediction = model.predict(pd_input)\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa42e43-08bc-4df5-9d55-a2e83327b919",
   "metadata": {
    "tags": []
   },
   "source": [
    "Call the deployment function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da040774-e874-4bc9-8a16-30694f3c4ca0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = [*cat_cols, *num_cols]\n",
    "deploy_model(session, \"predict_response\", f\"@{sp_udf_stage}/bank/udf/\", \"rfc_bank_model.joblib\" ,sp_dict['model_path'], features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dd57e4-6933-42d3-9af9-bdde88842c20",
   "metadata": {},
   "source": [
    "Test the UDF with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf8c866-93d1-4202-93eb-aad2ecd4afcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate a list of columns to use with the UDF\n",
    "input_cols = [F.col(col) for col in features]\n",
    "\n",
    "df_response_scores = df_bank_marketing.select(*input_cols, F.col(target_col), F.call_function(\"predict_response\", *input_cols).alias('PREDICTION'))\n",
    "df_response_scores.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa0948d-9e9c-4bcf-97aa-411935f3a3f4",
   "metadata": {},
   "source": [
    "Using the crosstab method allows us to do a quick confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fda319-87ca-44b8-8688-f7dda5be268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response_scores.crosstab(\"Y\", \"PREDICTION\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a820c21d-390b-4a9c-a759-134d500b1ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9811fa8d-3ab1-46be-9bad-337125dcbbd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
