{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of loging non Snowpark ML models into the Snowpark Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed Snowflake modules\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.ml.registry import Registry\n",
    "\n",
    "# Common modules for all examples\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Snowflake\n",
    "\n",
    "This example is using the connections.toml file to connect to Snowflake. You can read more at https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-connect#connecting-using-the-connections-toml-file how to set it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONNECTION_NAME = 'MY SNOWFLAKE CONNECTION' # Name of the connection in connections.toml to be used to connect to Snowflake\n",
    "DATABASE_NAME = 'SNOWPARK_DEMO_DB' # Database to use for data\n",
    "DATABASE_SCHEMA = 'SOURCE_DATA' # Name of schema to store data in and where wource data is\n",
    "FULLY_QUALIFIED_NAME = f\"{DATABASE_NAME}.{DATABASE_SCHEMA}\"\n",
    "\n",
    "snf_session = Session.builder.config(\"connection_name\", CONNECTION_NAME).create()\n",
    "snf_session.use_schema(FULLY_QUALIFIED_NAME)\n",
    "snf_session.get_fully_qualified_current_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "snowml_registry = Registry(snf_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn\n",
    "\n",
    "Train a RandomForestRegressor model within a pipline and log the fitted pipeline into the Snowpark Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKLearn Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some data to use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the diamond dataset\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/tidyverse/ggplot2/882584f915b23cda5091fb69e88f19e8200811bf/data-raw/diamonds.csv\", sep=',')\n",
    "\n",
    "# rename table to table_pct so we do not have any issues with selecting teh column when using data in snowflake\n",
    "data.rename(columns={'table': 'table_pct'}, inplace=True)\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define categorical and numerical columns, create X and y datastest and split them into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = [\"cut\", \"color\", \"clarity\"]\n",
    "NUMERICAL_COLUMNS = [\"carat\", \"depth\", \"table_pct\", \"x\", \"y\", \"z\"]\n",
    "X = data.drop([\"price\"], axis=1)\n",
    "y = data.price\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.9, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Pipeline that do preprocessing and then fit a RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categories = [\n",
    "    np.array([\"Ideal\", \"Premium\", \"Very Good\", \"Good\", \"Fair\"]), # cut\n",
    "    np.array(['D', 'E', 'F', 'G', 'H', 'I', 'J']), # color\n",
    "    np.array([\"IF\", \"VVS1\", \"VVS2\", \"VS1\", \"VS2\", \"SI1\", \"SI2\", \"I1\", \"I2\", \"I3\"]), # clarity\n",
    "]\n",
    "\n",
    "cat_transformer = Pipeline(steps=[\n",
    "        ('oe', OrdinalEncoder(categories=categories))\n",
    "    ])\n",
    "\n",
    "num_transformer = Pipeline(steps=[\n",
    "        ('scaler', MinMaxScaler(clip=True))\n",
    "    ])\n",
    "\n",
    "# Combine into a column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        ('num', num_transformer, NUMERICAL_COLUMNS),\n",
    "        ('cat', cat_transformer, CATEGORICAL_COLUMNS),\n",
    "    ],  verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "\n",
    "# Create a pipeline with the column transformer and training of a Random Forrest Classifier\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('classifier', RandomForestRegressor(n_jobs=-1))])\n",
    "\n",
    "# Train\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the MAPE on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(f\"Mean absolute percentage error: {mape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every model saved in the Snowpark Model Registry needs a unique name within the schema it is saved in, a model name can have multiple versions where each version needs a unique name (within the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skl_model_name = \"skl_diamonds\"\n",
    "skl_version_name = 'V1'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log the SKLearn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skl_mv = snowml_registry.log_model(model=pipeline, \n",
    "                                   model_name = skl_model_name, \n",
    "                                   version_name = skl_version_name,\n",
    "                                   sample_input_data = X_train.head(),\n",
    "                                   metrics = {\"test_mape\": mape},\n",
    "                                   comment='SKLearn pipline'\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check what functions we have avalible by using **show_functions()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skl_mv.show_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Snowpark DataFrame to test the deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snf_test_df = snf_session.create_dataframe([[0.23, 'Ideal', 'E', 'SI2', 61.5, 55.0, 3.95, 3.98, 2.43]]\n",
    "                                           , schema=['carat', 'cut', 'color', 'clarity', 'depth', 'table_pct', 'x', 'y', 'z'])\n",
    "snf_test_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USe the model on the Snowpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skl_mv.run(snf_test_df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using CustomModel\n",
    "https://docs.snowflake.com/en/developer-guide/snowpark-ml/reference/latest/model#snowflake-ml-model-custom-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed additional Snowpark ML modules\n",
    "from snowflake.ml.model import custom_model\n",
    "from snowflake.ml.model import model_signature\n",
    "\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyCaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed PyCaret modules\n",
    "from pycaret.classification import ClassificationExperiment, predict_model, load_model\n",
    "from pycaret.datasets import get_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by running a Classification Experiment using the Juice dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data('juice')\n",
    "\n",
    "cl_exp = ClassificationExperiment()\n",
    "cl_exp.setup(data, target='Purchase', session_id=123)\n",
    "best_model = cl_exp.compare_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to log the model/experiment into the Snowpark Model Registry we need to \n",
    "1) Serilize the model into a file\n",
    "2) Create a CustomModel class\n",
    "\n",
    "Start by saving the model as afile using the *save_model* method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model as file\n",
    "cl_exp.save_model(best_model, \"juice_best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set up a file structure to be used for the CustomModel and to move the saved file into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ARTIFACTS_DIR = \"/tmp/pycaret/\"\n",
    "# Create the directory where we will move the file\n",
    "os.makedirs(os.path.join(ARTIFACTS_DIR, \"model\"), exist_ok=True)\n",
    "# Move the saved model into the directory\n",
    "shutil.move('juice_best_model.pkl', os.path.join(ARTIFACTS_DIR, 'model',  'juice_best_model.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to create a CustomModel class that will be used in Snowflake when calling the methods/functions of the model. In this case we will only support the *predict* function, but if wanted to support addtional functions we would specifiy those as methonds of our class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the class\n",
    "class PyCaretModel(custom_model.CustomModel):\n",
    "    # The init function is used to load the model file\n",
    "    def __init__(self, context: custom_model.ModelContext) -> None:\n",
    "        super().__init__(context)\n",
    "        # The model is saved with .pkl prefix, and the filename will be part of the properties of the ModelContext\n",
    "        # we craete when logging it to Snowflake. Since PyCaret load function does not support using the prefix we \n",
    "        # need to remove it from the name\n",
    "        model_dir = self.context.path(\"model_file\")[:-4]\n",
    "        # Load the model\n",
    "        self.model = load_model(model_dir, verbose=False)\n",
    "        # When running this model in Snowflake it will use a WH and we do not have access to /var/ on the nodes so\n",
    "        # we need to change to a directory we have access to in this case /tmp/\n",
    "        self.model.memory='/tmp/' \n",
    "\n",
    "    @custom_model.inference_api\n",
    "    def predict(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        model_output = predict_model(self.model, data=X)\n",
    "        res_df = pd.DataFrame({\"prediction_label\": model_output['prediction_label'], \"prediction_score\": model_output['prediction_score']})\n",
    "        \n",
    "        return res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this CustomModel class every time we want to log a PyCaret ClassificationExperiment to Snowflake.\n",
    "\n",
    "Before logging the model we need to define the ModelContext, that will point to the artifatcs, file, needed when using the model in Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pycaret_mc = custom_model.ModelContext(\n",
    "\tmodels={ # This should be for models that is supported by Model Registry\n",
    "\t},\n",
    "\tartifacts={ # Everything not supported needs to be here\n",
    "\t\t'model_file': os.path.join(ARTIFACTS_DIR, \"model\",  'juice_best_model.pkl'),\n",
    "\t}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a new Model object and test that with some data, we will save the predictions into a Pandas DataFrame so we can use it later when generating a model signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pycaret_model = PyCaretModel(pycaret_mc)\n",
    "\n",
    "new_data = data.copy().drop('Purchase', axis=1)\n",
    "\n",
    "output_pd = my_pycaret_model.predict(new_data)\n",
    "output_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every model saved in the Snowpark Model Registry needs a unique name within the schema it is saved in, a model name can have multiple versions where each version needs a unique name (within the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"pycaret_juice\"\n",
    "version_name = \"v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before logging teh model we need to provide a Model Signauture. A Model Signature can be created using sample data for the input and output and we can use the *model_signature.infer_signature* function to generate it from the \n",
    "data.\n",
    "\n",
    "In thsi case we can use the **new_data** Pandas DataFrame as the input_data and **output_pd** Pandas DataFrame as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to create signature since \n",
    "predict_sign = model_signature.infer_signature(input_data=new_data.sample(100), output_data=output_pd.sample(100))\n",
    "predict_sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now log the model,  we will use the moel signature for the predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_mv = snowml_registry.log_model(\n",
    "    my_pycaret_model,\n",
    "    model_name=model_name,\n",
    "    version_name=version_name,\n",
    "    conda_dependencies=[\"pycaret\"],\n",
    "    signatures={\"predict\": predict_sign},\n",
    "    comment = 'PyCaret ClassificationExperiment using the CustomModel API'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_snowflake = snf_session.write_pandas(new_data,\"pycaret_input_data\", auto_create_table=True, overwrite=True, quote_identifiers=False )\n",
    "new_data_snowflake.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_mv.run(new_data_snowflake, function_name='predict').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "SELECT \n",
    " pycaret_juice!predict(*) as predict_dict,\n",
    " predict_dict['prediction_label']::text as prediction_label,\n",
    " predict_dict['prediction_score']::double as prediction_score\n",
    "from pycaret_input_data;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowml_latest_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
