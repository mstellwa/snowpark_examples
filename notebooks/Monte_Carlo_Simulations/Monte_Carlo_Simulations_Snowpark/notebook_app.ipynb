{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d29f56b5-bf13-493e-88e0-9eb34982a307",
   "metadata": {
    "name": "cell4",
    "collapsed": false,
    "resultHeight": 209
   },
   "source": "Before running the notebook you need to add the *Tick History* dataset from *Factset* to your account, via the Marketplace, the shared database must be named *tick_history*.\n\nYou also need to add the following Python packages using the **Packages** drop down:\n* `snowflake.core`\n* `scipy`\n* `matplotlib`"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "imports",
    "resultHeight": 38,
    "collapsed": false
   },
   "source": "# Import python packages\nimport streamlit as st\n\n# Snowpark\nimport snowflake.snowpark as S\nfrom snowflake.snowpark import Column\nimport snowflake.snowpark.types as snow_types\nimport snowflake.snowpark.functions as snow_funcs\nfrom snowflake.snowpark import Window\nfrom snowflake.snowpark.context import get_active_session\n\n# Snowflake Python API\nfrom snowflake.core import Root\nfrom snowflake.core.database import Database\nfrom snowflake.core.schema import Schema\nfrom snowflake.core.stage import Stage, StageEncryption, StageDirectoryTable\nfrom snowflake.core import Root\n\n# Additional third-party libraries\nimport numpy as np\nfrom typing import Tuple,Iterable\nfrom scipy.stats import norm\n\n# Get the session ie the user and role that is running this notebook\nsession = get_active_session()\n# Print the version of Snowpark we are using\nprint(f\"Using Snowpark: {S.__version__}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "02746c2d-821e-4893-80b4-24840e6d775a",
   "metadata": {
    "language": "python",
    "name": "context",
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Where we will store the aggregated ticker data\ndb_name = \"SNOWPARK_DEMO_DB\"\nschema_name = \"MCS_SCHEMA\"\nstage_name = \"UDF_STAGE\"",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c68cbea3-6b1c-428b-99cd-834b42f5ae3b",
   "metadata": {
    "name": "cell23",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Start by creating the nessecary Snowflake objects, this is only needed once."
  },
  {
   "cell_type": "code",
   "id": "0376a666-a8ef-47f5-9e83-b42c5cfee3f5",
   "metadata": {
    "language": "python",
    "name": "object_creation",
    "codeCollapsed": false,
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Create Database & Schema\nroot = Root(session)\ndemo_db = Database(name=db_name)\ndemo_db = root.databases.create(demo_db, mode='if_not_exists')\n\ndemo_schema = Schema(name=schema_name)\ndemo_schema = demo_db.schemas.create(demo_schema, mode='or_replace')\n\n# Set context\nsession.use_schema(f'{db_name}.{schema_name}')\n\n# Create Stages\nudf_stage = Stage(\n  name=stage_name,\n  encryption=StageEncryption(type=\"SNOWFLAKE_SSE\"), \n  directory_table=StageDirectoryTable(enable=True)\n)\n\nudf_stage = demo_schema.stages.create(udf_stage, mode='or_replace')\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "63054bd0-2a85-496e-8092-7f7003b7549a",
   "metadata": {
    "language": "python",
    "name": "set_context",
    "resultHeight": 83,
    "collapsed": false
   },
   "outputs": [],
   "source": "session.use_schema(f'{db_name}.{schema_name}')\nprint(f\"Current schema: {session.get_fully_qualified_current_schema()}\")\nprint(f\"Current role: {session.get_current_role()}\")\nprint(f\"Current warehouse: {session.get_current_warehouse()}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "daca61e7-37cb-4365-9dc2-8c995d894e04",
   "metadata": {
    "name": "cell9",
    "collapsed": false,
    "resultHeight": 294
   },
   "source": "The source table used, sample dataset shared by Factset, contains tick data for the previous six months for the following symbols:\n* IBM-USA\n* AMZN-USA\n* FDS-USA\n* MSFT-USA\n* AAPL-USA\n* META-USA\n\nFor the demo we will use the closing price for each day for one symbol so we will aggregate the tick data into one row for each symbol and day with the close price. The definition of close price is the last value for the day and we will also convert the last_date and last_time values into date and time."
  },
  {
   "cell_type": "code",
   "id": "5f51e618-9b03-4b15-bb4e-011fd72b9a6c",
   "metadata": {
    "language": "python",
    "name": "generate_closing_price",
    "resultHeight": 0,
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Create a dataframe with closing prices\ndf_closing_prices_trans = (session.table('tick_history.public.th_sf_mktplace')\n                                # Only use Equity trades\n                                .filter((snow_funcs.col('last_price').is_not_null())\n                                            & (snow_funcs.col('msg_type') == 0) # Only trades\n                                            & (snow_funcs.col('SECURITY_TYPE') == 1) # only Equity\n                                       )\n                                # Cast last_date to Dates and last_time to Time\n                                .with_columns(['trade_date', 'trade_time']\n                                                      ,[snow_funcs.to_date(snow_funcs.to_char(snow_funcs.col('last_date'))\n                                                                           , snow_funcs.lit('YYYYMMDD'))\n                                                       ,snow_funcs.to_time(snow_funcs.to_char(snow_funcs.col('last_time'))\n                                                                           , snow_funcs.lit('hhmissff'))])\n                                .select('ticker', 'trade_date', 'trade_time', 'last_price', 'last_vol')\n                                # Get the last value for each day as the close price\n                                .with_column('closing_price'\n                                             , snow_funcs.last_value(snow_funcs.col('last_price'))\n                                                            .over(Window.partition_by(snow_funcs.col('ticker'), snow_funcs.col('trade_date'))\n                                                                        .order_by(snow_funcs.col('trade_time'))\n                                                                 )\n                                            )\n                                # Aggregate into one row for each symbol and day\n                                .group_by(snow_funcs.col('TICKER'), snow_funcs.col('TRADE_DATE'))\n                                .agg(snow_funcs.max(snow_funcs.col('CLOSING_PRICE')).alias('CLOSING_PRICE'))\n                        )\n# Save the data into a table so we can use this for multiple purposes\ndf_closing_prices_trans.write.save_as_table(\"closing_prices\", mode=\"overwrite\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ec0a72f1-b638-4a34-b148-779828b63678",
   "metadata": {
    "name": "cell1",
    "collapsed": false,
    "resultHeight": 351
   },
   "source": "If we want this to run every time we get new data in tick_history.public.th_sf_mktplace we can \ninstead save the dataframe as a dynamic table, below will create a Dynamic Table that will check once an hour if there is new data and then incremental update ie just add new rows.\n```\ndf_closing_prices_trans.create_or_replace_dynamic_table(\"closing_prices_dt\"\n                                                        , warehouse=\"COMPUTE_WH\"\n                                                        , lag=\"1 hour\"\n                                                        , mode=\"overwrite\"\n                                                        , refresh_mode=\"INCREMENTAL\"\n                                                        , initialize=\"ON_CREATE\")\n\n```\n\nCreate a Snowpark dataframe filtered to only show close price for IBM for 2024-05-01 and forward.\n\nDisplay 20 rows of it."
  },
  {
   "cell_type": "code",
   "id": "d332e2f1-fb13-4f56-a56c-233428681023",
   "metadata": {
    "language": "python",
    "name": "create_closing_price_dataframe",
    "resultHeight": 439,
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "df_closing = (session.table(\"closing_prices\")\n                        .filter((snow_funcs.col(\"TICKER\") == 'IBM') & (snow_funcs.col(\"TRADE_DATE\") >= '2022-05-01'))\n                        .sort(snow_funcs.col(\"TRADE_DATE\"))\n             )\n\ndf_closing.sort(snow_funcs.col('TICKER'), snow_funcs.col('TRADE_DATE')).limit(20)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "843ecdee-2f6b-481d-97f9-11ba15e6337b",
   "metadata": {
    "name": "cell19",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Plot the daily closing price over time"
  },
  {
   "cell_type": "code",
   "id": "ef48c60f-8a0d-4d03-82fb-023a555ce3d9",
   "metadata": {
    "language": "python",
    "name": "visulize_closing_price",
    "resultHeight": 491,
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "df_closing.to_pandas().plot(x=\"TRADE_DATE\", figsize=(6,2))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d8f56dd0-06f5-4d0d-8fc8-dba46d4653c2",
   "metadata": {
    "name": "cell24",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Set the number of days we are going to simulate the strock price and the number of simulations by day"
  },
  {
   "cell_type": "code",
   "id": "d49c77c2-2d75-4a92-8165-ff164dc86d85",
   "metadata": {
    "language": "python",
    "name": "set_simulations_days",
    "resultHeight": 0,
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "n_sim_runs = 10000 # intervals\nn_days = 30 # iterations",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3ac9b64d-0be1-4d0f-b6e3-71971fa4f69f",
   "metadata": {
    "name": "cell25",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "A helper function to create the Snowpark Dataframe logic for calculate precent change"
  },
  {
   "cell_type": "code",
   "id": "48035462-bd4a-46bc-a391-6d53ae5ef0ef",
   "metadata": {
    "language": "python",
    "name": "pct_change_def",
    "codeCollapsed": false,
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "def pct_change(indx_col: Column, val_col: Column):\n    return ((val_col - snow_funcs.lag(val_col, 1).over(Window.orderBy(indx_col))) / snow_funcs.lag(val_col, 1).over(Window.orderBy(indx_col)))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9d26beed-b361-4e5f-8a5d-f8997a36dc51",
   "metadata": {
    "name": "cell26",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Calculate historical log returns using the precent change logic"
  },
  {
   "cell_type": "code",
   "id": "1fb809d0-4363-4bb0-906d-65e7ec9551e2",
   "metadata": {
    "language": "python",
    "name": "calculate_log_returns",
    "codeCollapsed": false,
    "resultHeight": 329,
    "collapsed": false
   },
   "outputs": [],
   "source": "df_log_returns = df_closing.select(snow_funcs.col(\"TRADE_DATE\"), snow_funcs.col(\"CLOSING_PRICE\")\n                            ,snow_funcs.last_value(snow_funcs.col(\"CLOSING_PRICE\")).over(Window.orderBy(\"TRADE_DATE\")).as_(\"LAST_CLOSE\")\n                           ,snow_funcs.call_function(\"LN\", (snow_funcs.lit(1) + pct_change(snow_funcs.col(\"TRADE_DATE\"), snow_funcs.col(\"CLOSING_PRICE\")))).as_(\"log_return\"))\ndf_log_returns.sort(\"TRADE_DATE\").show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f6f8808c-59f2-42d2-b57b-ad26104798ea",
   "metadata": {
    "name": "cell27",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "Calculate the drift as the mean for `log returns - (variance of log returns/2)` and also return the last closing price and standard dev for it. We will pull back the values and store that in local variables."
  },
  {
   "cell_type": "code",
   "id": "9b508af4-a816-4450-b8fd-24d5b1cb5218",
   "metadata": {
    "language": "python",
    "name": "calculate_parameters",
    "codeCollapsed": false,
    "resultHeight": 83,
    "collapsed": false
   },
   "outputs": [],
   "source": "params = (df_log_returns.select(snow_funcs.mean(\"LOG_RETURN\").as_(\"u\")\n                            , snow_funcs.variance(\"LOG_RETURN\").as_(\"var\")\n                            , snow_funcs.stddev(\"LOG_RETURN\").as_(\"std_dev\")\n                            ,snow_funcs.max(snow_funcs.col(\"last_close\")).as_(\"LAST_CLOSE\"))\n        .with_column(\"drift\", (snow_funcs.col(\"u\")-(snow_funcs.lit(0.5)*snow_funcs.col(\"var\"))))\n        .select(\"std_dev\", \"drift\", \"last_close\")\n        ).collect()\n\nstd_dev = params[0]['STD_DEV']\ndrift = params[0]['DRIFT']\nlast_close = params[0]['LAST_CLOSE']\n\nprint(f\"Last close: {last_close}\")\nprint(f\"Drift: {drift}\")\nprint(f\"Std Dev: {std_dev}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8b539994-8ef2-47de-92ab-207cd2f1603d",
   "metadata": {
    "name": "cell28",
    "collapsed": false,
    "resultHeight": 185
   },
   "source": "We want to calculate a simulated closing price for each iteration and day, one way to do this would be to loop through days and iterations and calculate it. This will require us to make sure we have enough memory etc and it also is a sequential process.  \n\nBy generate two dataframes, one with one row for each day and one  with one row for each simulation, we can push down the computation to Snowflake using the power of the SQL engine and compute provided to do this on scale."
  },
  {
   "cell_type": "code",
   "id": "f80d0abc-364d-4f2c-968d-2ad5ad664190",
   "metadata": {
    "language": "python",
    "name": "generate_sims_days_dfs",
    "resultHeight": 0,
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "id_generator = snow_funcs.row_number().over(Window.order_by(snow_funcs.seq4()))\n\ndf_days = session.generator(id_generator.as_(\"day_id\") ,rowcount=n_days)\ndf_sim_runs = session.generator(id_generator.as_(\"sim_run\") ,rowcount=n_sim_runs)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "51fef633-1cb3-445d-8ac3-4ed3251d375c",
   "metadata": {
    "name": "cell29",
    "collapsed": false,
    "resultHeight": 118
   },
   "source": "We will use Numpy norm.ppf to get the random value for the simulations, since Snowflake does not have that specifc random function we can use it in a Python UDF. By setting the parameter is_permanent to True the UDF is permanent and we can later reuse it for other purposes or other users can use it (using Python, Scala, Java or SQL)."
  },
  {
   "cell_type": "code",
   "id": "13e15198-934a-4f6f-a767-ef79337157b8",
   "metadata": {
    "language": "python",
    "name": "norm_ppf_udf",
    "resultHeight": 0,
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "@snow_funcs.udf(name=\"norm_ppf\", is_permanent=True, replace=True, packages=[\"scipy\"]\n                , stage_location=stage_name, session=session)\ndef norm_ppf(pd_series: snow_types.PandasSeries[float]) -> snow_types.PandasSeries[float]:\n    return norm.ppf(pd_series)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b7871b89-370e-463c-8f3c-d56151d09f27",
   "metadata": {
    "name": "cell30",
    "collapsed": false,
    "resultHeight": 92
   },
   "source": "Calculate the daily return using the UDF for norm ppf, for each day and simulation. We are cross joing the dataframes with days and simulations to create a new dataframe that has one row for each day and simulation combination with the daily return. This would be equal to loop through days and iterations."
  },
  {
   "cell_type": "code",
   "id": "7ba9bac1-e76d-4ee6-87fc-78d4292b9ea4",
   "metadata": {
    "language": "python",
    "name": "calculate_daily_return",
    "resultHeight": 438,
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "df_daily_returns = (df_days.join(df_sim_runs)\n                            .select(\"day_id\", \"sim_run\"\n                                    , snow_funcs.exp(snow_funcs.lit(drift) + snow_funcs.lit(std_dev) \n                                                        *  snow_funcs.call_function(\"norm_ppf\", snow_funcs.uniform(0.0,1.0,snow_funcs.random()))).as_(\"daily_return\"))\n                            .sort(snow_funcs.col(\"DAY_ID\"), snow_funcs.col(\"sim_run\"))\n                    )\ndf_daily_returns.limit(20)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dfbf76e9-308d-4907-a78e-440a0d43b91b",
   "metadata": {
    "name": "cell31",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Generate a day 0 row with return 1.0 as the starting point"
  },
  {
   "cell_type": "code",
   "id": "29be9d1b-5f52-4e38-81a5-fcfa01abdf76",
   "metadata": {
    "language": "python",
    "name": "generate_start_day",
    "resultHeight": 426,
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\ndf_day_0 = session.generator(snow_funcs.lit(0).as_(\"DAY_ID\"),  \n                                snow_funcs.row_number().over(Window.order_by(snow_funcs.seq4())).as_(\"SIM_RUN\")\n                                , snow_funcs.lit(1.0).as_(\"DAILY_RETURN\"), rowcount=n_sim_runs)\ndf_day_0.limit(10)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a1eb259a-d9d8-4026-9ae2-d674f65ae541",
   "metadata": {
    "name": "cell32",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Union the dataframe so we only have one dataset with all rows"
  },
  {
   "cell_type": "code",
   "id": "72316329-236f-4825-9278-44ccee36ea77",
   "metadata": {
    "language": "python",
    "name": "generate_simulations_df",
    "resultHeight": 427,
    "collapsed": false
   },
   "outputs": [],
   "source": "df_simulations = df_day_0.union_all(df_daily_returns)\ndf_simulations.filter(snow_funcs.col(\"SIM_RUN\") == 1).sort(\"DAY_ID\", \"SIM_RUN\").limit(10)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "de004fa9-e980-45b2-a698-358a9ec7d949",
   "metadata": {
    "name": "cell45",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "Create a User Defined Table Function (UDTF) for calulating the simulated return, by using a UDTF we can keep track of the previous claculated value for each row and we can slo use Snowflake's capabilities in running it distirbutet."
  },
  {
   "cell_type": "code",
   "id": "6c0bec29-a6ec-4d47-939e-b699af51177f",
   "metadata": {
    "language": "python",
    "name": "calc_return_udtf_py",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "@snow_funcs.udtf(name=\"calc_return_udtf\", is_permanent=True, replace=True\n        ,packages=[\"typing\"],  output_schema=['SIM_CLOSE'], input_types=[snow_types.FloatType(), snow_types.FloatType()]\n        ,stage_location=\"UDF_STAGE\", session=session)\nclass calc_return_handler:\n    def __init__(self) -> None:\n        self.prev_close = 0.0\n    def process(self, last_close, daily_return) -> Iterable[Tuple[float]]:\n        # First call we will use the last_call values,\n        # for the rest prev_close\n        if self.prev_close == 0.0:\n            self.prev_close = last_close\n        \n        sim_close = self.prev_close * daily_return\n        self.prev_close = sim_close # Keep track of the calculated return so we can use it for the next row\n        yield (sim_close,)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "48ed963e-45d4-4412-a7cb-a9055068ddaf",
   "metadata": {
    "name": "cell33",
    "collapsed": false,
    "resultHeight": 67
   },
   "source": "Apply the UDTF on the rows, by using **partition_by** we can distrubute the calculation by simulations. In this example we have only one symbol so we just distribute by SIM_RUN, if we had more symbols we would also distrubute by each symbol."
  },
  {
   "cell_type": "code",
   "id": "a75031a4-4a67-4797-8b8f-c4243a15394e",
   "metadata": {
    "language": "python",
    "name": "simulate_close",
    "resultHeight": 427,
    "collapsed": false
   },
   "outputs": [],
   "source": "df_sim_close = (df_simulations\n                    .with_column(\"SIM_CLOSE\", snow_funcs.call_table_function(\"calc_return_udtf\", snow_funcs.lit(last_close)\n                                                                                , snow_funcs.col(\"DAILY_RETURN\")).over(partition_by=\"SIM_RUN\", order_by=\"DAY_ID\"))\n            )\n\ndf_sim_close.filter(snow_funcs.col(\"SIM_RUN\") == 1).sort(\"DAY_ID\", \"SIM_RUN\").limit(10)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "da513743-89df-443b-aeb4-df8e1f1210ef",
   "metadata": {
    "name": "cell2",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "We can save the simulations into a table to be used by others"
  },
  {
   "cell_type": "code",
   "id": "43c9249f-c78a-434b-a243-dabd3de0aecb",
   "metadata": {
    "language": "python",
    "name": "py_save_simulations",
    "resultHeight": 0,
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "df_sim_close.write.save_as_table(\"ibm_30d_simulations\", mode=\"overwrite\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "db6d84c5-ad3d-4d97-9e35-fc33f36cb378",
   "metadata": {
    "name": "cell3",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Plot the min, mean and max for each day"
  },
  {
   "cell_type": "code",
   "id": "c7ef04b6-481f-446d-a5dd-7eb0482fc1f9",
   "metadata": {
    "language": "python",
    "name": "visulaize_sim_runs",
    "codeCollapsed": false,
    "resultHeight": 83,
    "collapsed": false
   },
   "outputs": [],
   "source": "pd_local = (session.table(\"ibm_30d_simulations\")\n                    .group_by('DAY_ID')\n                    .agg(snow_funcs.max(snow_funcs.col('SIM_CLOSE')).alias(\"MAX_CLOSE\")\n                            , snow_funcs.mean(snow_funcs.col('SIM_CLOSE')).alias(\"MEAN_CLOSE\")\n                            , snow_funcs.min(snow_funcs.col('SIM_CLOSE')).alias(\"MIN_CLOSE\"))\n                    .sort('DAY_ID')\n                    ).to_pandas()\n\npd_local.plot(x='DAY_ID', figsize=(6,2))\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7d39a016-7561-4601-b10e-e350a7cf89bd",
   "metadata": {
    "name": "cell42",
    "collapsed": false,
    "resultHeight": 41
   },
   "source": "Get the mean, Quantile (5%) and Quantile (95%)"
  },
  {
   "cell_type": "code",
   "id": "f03dc6bc-8222-489d-ac7c-05cd36c70d0b",
   "metadata": {
    "language": "python",
    "name": "get_metrics_of_the_runs",
    "resultHeight": 83,
    "codeCollapsed": false,
    "collapsed": false
   },
   "outputs": [],
   "source": "metrics = df_sim_close.select(snow_funcs.round(snow_funcs.mean(snow_funcs.col(\"SIM_CLOSE\")), 2)\n                                    , snow_funcs.round(snow_funcs.percentile_cont(0.05).within_group(\"SIM_CLOSE\"), 2)\n                                    , snow_funcs.round(snow_funcs.percentile_cont(0.95).within_group(\"SIM_CLOSE\"), 2)).collect()\nprint(f\"Mean price: {metrics[0][0]}\")\nprint(f\"Quantile (5%): {metrics[0][1]}\")\nprint(f\"Quantile (95%): {metrics[0][2]}\")",
   "execution_count": null
  }
 ]
}