{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5729cca3-231f-44b2-a44b-69c23a841081",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Demo of Snowpark end-to-end Machine Learning\n",
    "\n",
    "Purpose of this demo is to showcase how the Snowpark ML library can be used for end to end Machine Learning\n",
    "\n",
    "The dataset is from https://archive-beta.ics.uci.edu/dataset/222/bank+marketing  \n",
    "**Run 00_Load_demo_data.ipynb to upload the Parquet files used for this Notebook**\n",
    "\n",
    "It has the following columns:  \n",
    "**bank client data**:  \n",
    "1 - age (numeric)  \n",
    "2 - job : type of job (categorical: \"admin.\",\"unknown\",\"unemployed\",\"management\",\"housemaid\",\"entrepreneur\",\"student\",  \n",
    "                                   \"blue-collar\",\"self-employed\",\"retired\",\"technician\",\"services\")   \n",
    "3 - marital : marital status (categorical: \"married\",\"divorced\",\"single\"; note: \"divorced\" means divorced or widowed)  \n",
    "4 - education (categorical: \"unknown\",\"secondary\",\"primary\",\"tertiary\")  \n",
    "5 - default: has credit in default? (binary: \"yes\",\"no\")  \n",
    "6 - balance: average yearly balance, in euros (numeric)   \n",
    "7 - housing: has housing loan? (binary: \"yes\",\"no\")  \n",
    "8 - loan: has personal loan? (binary: \"yes\",\"no\")  \n",
    "**related with the last contact of the current campaign**:  \n",
    "9 - contact: contact communication type (categorical: \"unknown\",\"telephone\",\"cellular\")   \n",
    "10 - day: last contact day of the month (numeric)  \n",
    "11 - month: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", ..., \"nov\", \"dec\")  \n",
    "12 - duration: last contact duration, in seconds (numeric)  \n",
    "**other attributes**:  \n",
    "13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)  \n",
    "14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)  \n",
    "15 - previous: number of contacts performed before this campaign and for this client (numeric)  \n",
    "16 - poutcome: outcome of the previous marketing campaign (categorical: \"unknown\",\"other\",\"failure\",\"success\")  \n",
    "**Output variable (desired target)**:  \n",
    "17 - y - has the client subscribed a term deposit? (binary: \"yes\",\"no\")  \n",
    "\n",
    "Start by importing needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63379898-fbc2-4892-828e-aac347203f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c0319e-3237-437a-a19b-83c3e5411a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports \n",
    "import snowflake.snowpark as S\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import types as T\n",
    "from snowflake.snowpark import Window\n",
    "\n",
    "from snowflake.ml.version  import VERSION as snowml_version\n",
    "\n",
    "import snowflake.ml.modeling.preprocessing as pp\n",
    "from snowflake.ml.modeling.pipeline import Pipeline\n",
    "from snowflake.ml.modeling.ensemble import RandomForestClassifier\n",
    "from snowflake.ml.modeling.metrics import correlation, precision_recall_fscore_support, accuracy_score, confusion_matrix\n",
    "from snowflake.ml.registry import model_registry\n",
    "\n",
    "import json\n",
    "\n",
    "# Make sure we do not get line breaks when doing show on wide dataframes\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "\n",
    "import pandas as pd\n",
    "import sqlparse\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sn\n",
    "%matplotlib inline\n",
    "\n",
    "# Print the version of Snowpark we are using\n",
    "print(f\"Using Snowpark: {S.__version__}\")\n",
    "print(f\"Using Snowflake ML: {snowml_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172d5b96-661f-4447-9818-12f74713433d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93861747-9998-42e9-81d8-8eea613c950a",
   "metadata": {},
   "source": [
    "Helper functions for nicer printing of Snowpark dataframe schema and SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a619ad-deb3-4494-b81b-9421da9c5f14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions for nicer printing\n",
    "def print_sql(df):\n",
    "    for query in df.queries['queries']:\n",
    "        print(sqlparse.format(query, reindent=True))\n",
    "\n",
    "def print_schema(df):\n",
    "    print(\"schema:\")\n",
    "    for col in df.schema.fields:\n",
    "        print(f\" |-- {col.name}: {col.datatype} (Nullable: {col.nullable})\")\n",
    "\n",
    "def shape(df):\n",
    "    return (df.count(), len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f41d1-4a3d-43b1-9814-b2057e0896e8",
   "metadata": {},
   "source": [
    "Connect to Snowflake\n",
    "\n",
    "This example is using a JSON file with the following structure\n",
    "```\n",
    "{\n",
    "    \"account\":\"MY SNOWFLAKE ACCOUNT\",\n",
    "    \"user\": \"MY USER\",\n",
    "    \"password\":\"MY PASSWORD\",\n",
    "    \"role\":\"MY ROLE\",\n",
    "    \"warehouse\":\"MY WH\",\n",
    "    \"database\":\"MY DB\",\n",
    "    \"schema\":\"MY SCHEMA\"\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68234ce-6537-4f9e-93cd-66f29e41fc1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('../creds.json') as f:\n",
    "    connection_parameters = json.load(f)\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "print(\"Current role: \" + session.get_current_role() + \", Current schema: \" + session.get_fully_qualified_current_schema() + \", Current WH: \" + session.get_current_warehouse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02012014-bf8d-4851-a303-6fd4e4d5249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "source_path = \"@SOURCE_FILES/BANK_MARKETING\" # Where the source parquet files are stored\n",
    "sp_udf_stage = \"BANK_STAGE\" # Name of the stage to used for storing the code for the SP and UDF , as well the trained model files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0bbd48-5d09-47f0-850e-af485990405a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Loading of source data\n",
    "### Loading Parquet files with inferring the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048afc37",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.sql(f\"ls {source_path}\").select('\"name\"').show(30, max_width=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4f167e-66c5-4df1-8b70-ab5acd4055da",
   "metadata": {},
   "source": [
    "Take a peak in the files, in order to read the file with a select as a Parquet file we need to create a temporary file format object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562355d5-fba0-403f-98af-c2ca90eb9e78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.sql(\"create or replace temp file format parq1 type='PARQUET'\").collect()\n",
    "session.sql(f\"select $1 from {source_path} (file_format=>parq1 )\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a45358f",
   "metadata": {},
   "source": [
    "Loading Parquet files with inferring the schema. When using **read.parquet** the file format object is created autamtically for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2492761e-5f90-4e27-93be-a936923d2b2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_reader = session.read.parquet(source_path)\n",
    "df_reader.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a898c018-229f-4f6a-b6d2-84d7509ae5c5",
   "metadata": {},
   "source": [
    "Looking at the SQL for the df_reader, it shows that we now will read directly from the stage every time we access the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40494bbf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_sql(df_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7bb8ba-5e2d-4367-8ae5-c4a0d4128291",
   "metadata": {},
   "source": [
    "Saving the dta into a physical table in Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce3aaf-a806-4044-a91b-04646a3c90db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.sql(\"DROP TABLE IF EXISTS bank_marketing_snowml\").collect()\n",
    "df_reader.copy_into_table(\"bank_marketing_snowml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da084808",
   "metadata": {},
   "source": [
    "## Create a Snowpark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd2db2-4830-426a-9a8a-0e1d9c224182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_bank_marketing = session.table(\"bank_marketing_snowml\")\n",
    "display(f\"Dataframe shape: {shape(df_bank_marketing)}\")\n",
    "df_bank_marketing.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9412ad-8bf1-414c-9cc3-c4c2ee2d86c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_sql(df_bank_marketing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c08dea6-a8ea-4227-a2b2-7250ada628db",
   "metadata": {},
   "source": [
    "## Data understanding\n",
    "\n",
    "Start with verifying datatypes, simple put we will treat charcter columns as categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87a9434-7719-47f3-8fa6-d37f170c6633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_schema(df_bank_marketing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cf2667-d23c-46fd-9d83-4771f1786f79",
   "metadata": {},
   "source": [
    "DAY is stored as a number but can be threaded as categorical, fixed number of days in months, and by changing the data type to character we will do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3c8b8b-19b3-4087-a3fd-b8281b94c55f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_bank_marketing_prep = df_bank_marketing.with_column(\"DAY\", F.to_varchar(F.col(\"DAY\"))).with_column_renamed(\"DEFAULT\", \"CREDIT_DEFAULT\")\n",
    "print_schema(df_bank_marketing_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1081a8dd-58bf-41df-b1ea-b59c4780b4e6",
   "metadata": {},
   "source": [
    "Get basic statistics about the categorical and numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5151d57-da39-4399-8fe0-20293f3d4c75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_bank_marketing_prep.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea09b2d9-d565-42c7-ab97-594ecc1e85be",
   "metadata": {},
   "source": [
    "Create variables with our categorical, numeric and target columns names so we can use them with encoders and scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b1e1e-1de7-4b80-9cc3-a10b406d0bfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_cols = [c.name for c in df_bank_marketing_prep.schema.fields if (type(c.datatype) == T.StringType) & (c.name != 'Y')]\n",
    "numeric_types = [T.DecimalType, T.LongType, T.DoubleType, T.FloatType, T.IntegerType]\n",
    "num_cols = [c.name for c in df_bank_marketing_prep.schema.fields if type(c.datatype) in numeric_types]\n",
    "target_col = \"Y\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc93ff-948e-47b7-bb5e-a665fcdf0630",
   "metadata": {},
   "source": [
    "Distribution of target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc31c3d-f546-41f4-a5e3-3b5f3973f1b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_bank_marketing_prep.group_by(target_col).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50fc992-84ed-4d2f-8f51-582163d3db3e",
   "metadata": {},
   "source": [
    "Frequency tables for each categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52afbf1e-b58f-4e27-93e2-1479baefec78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    display(df_bank_marketing_prep.select(F.count_distinct(col).as_(f\"{col} distinct values\")).show())\n",
    "    display(df_bank_marketing_prep.group_by(col).count()\\\n",
    "                                .select(col, (F.call_function(\"RATIO_TO_REPORT\", F.col(\"COUNT\")).over() * 100).as_(\"% observations\") )\\\n",
    "                                .sort(F.col(\"% observations\").desc()).show(31))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844b7a90-a815-4eb9-9015-e88b951071e5",
   "metadata": {},
   "source": [
    "Relationship between each of the categorical features and the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13778858-57b7-4ecd-99b8-10c97877c1d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    window = Window.partition_by(col)\n",
    "    display(df_bank_marketing_prep.group_by(col, F.col(target_col))\\\n",
    "                                .count()\\\n",
    "                                .select(col, F.col(target_col), (F.call_function(\"RATIO_TO_REPORT\", F.col(\"COUNT\")).over(window) * 100).as_(\"percentage\"))\\\n",
    "                                .pivot(target_col, ['no', 'yes']).agg(F.sum(\"percentage\")).show(50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f0a8d-e2b6-4609-ad25-450f20cf3de3",
   "metadata": {},
   "source": [
    "Check the correlation between all numeric variables using the correlation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc76c438-5d55-4c68-bc13-f9fc8d60ca6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr_matrix = correlation(df=df_bank_marketing_prep)\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dface0-d214-4929-9d4e-cce23b3801e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sn.heatmap(corr_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5dba3f-c67d-4aef-851e-6ac12c8e57b9",
   "metadata": {},
   "source": [
    "Have a look at the PDAYS columns, that is the number of days since last contact and if the customer has never been contacted it has -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b07397d-04db-4723-921e-207671c9d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_marketing_prep.group_by(\"PDAYS\").count().sort(F.col(\"COUNT\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddba214-fd08-4ad2-81bc-8fed00585334",
   "metadata": {},
   "source": [
    "Majority of the customers have never been contacted (have -1), what is the max and min values of it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39786c62-690f-4c38-a208-d3a7aaaf383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = df_bank_marketing_prep.select(F.min(\"PDAYS\").as_(\"MIN_VAL\"), F.max(\"PDAYS\").as_(\"MAX_VAL\")).collect()[0]\n",
    "min_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ac0600-a0d8-4195-9481-d1bfe3066040",
   "metadata": {},
   "source": [
    "Since the value range is rather wide we can bin it so we get less vales. There is multiple ways to this, but to make things simple we will create 20 equal width bins. For this we could use the WIDTH_BUCKET function in Snowflake, but we want to also give the bins names based on their range so we can create a function for dynamically generate the bins and the lables for them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8319e2-b5dd-476f-9465-c8291b826796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_bucketize(df, column, max_values=[],labels=[]):\n",
    "    condition = None\n",
    "    for idx, bucket in enumerate(labels):\n",
    "        if idx <= len(max_values) - 1:\n",
    "            if type(condition) == F.CaseExpr:\n",
    "                condition = condition.when(F.col(column) < F.lit(max_values[idx]), F.lit(bucket))\n",
    "            else:\n",
    "                condition = F.when(F.col(column) < F.lit(max_values[idx]), F.lit(bucket))\n",
    "        else:\n",
    "            condition = condition.otherwise(F.lit(bucket))\n",
    "    df = df.with_column(column + '_BUCKET', condition)\n",
    "    df = df.drop(column)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c164f476-4357-4da1-b0bc-1ef74f294781",
   "metadata": {},
   "source": [
    "We need to generate the max values for each bin and the lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34439df4-d8c6-4f11-b522-70b71bab7f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "increment = int(round((min_max['MAX_VAL'] - min_max['MIN_VAL']) / 20, ndigits=0))\n",
    "bin_vals = [0]\n",
    "bin_lables = ['NEVER']\n",
    "for idx, val in enumerate(range(1, min_max['MAX_VAL'], increment)):\n",
    "    bin_vals.append(val)\n",
    "    idx += 1\n",
    "    bin_lable = f'{bin_vals[idx-1]}-{bin_vals[idx]-1}'\n",
    "    bin_lables.append(bin_lable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f8ca82-27af-4bd5-874f-a78f2f3c9cd1",
   "metadata": {},
   "source": [
    "Call the function ti generate the bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d58bdd-5b87-454f-9d74-d49fed4117ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_marketing_binned_prep = manual_bucketize(df_bank_marketing_prep, 'PDAYS', bin_vals, \n",
    "                                          bin_lables)\n",
    "df_bank_marketing_binned_prep.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc6871-0f13-4e09-b4ee-617ad83795fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_sql(df_bank_marketing_binned_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ad86c1-37c3-4b67-902c-2ea28c149da2",
   "metadata": {},
   "source": [
    "We will use PDAYS_BIN instead of PDAYS so we will remove PDAYS form the numeric columns list and add PDAYS_BIN to teh categorical columns list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d445a4-7d8e-43c9-accf-1a6e71843f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols.append(\"PDAYS_BUCKET\")\n",
    "num_cols.remove(\"PDAYS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ce4342-59dc-4be3-aa4c-fcc2d66a5b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_marketing_binned_prep.write.save_as_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb388dd-ae86-4474-9fcb-1ea0957873bc",
   "metadata": {},
   "source": [
    "## Using snowml for preprocessing and training\n",
    "\n",
    "Snowml includes the possibility to create piplines and to have training of scikit-learn, XGBoost and Lightgbm models automatically pushed down to Snwoflake, including inference.\n",
    "\n",
    "We can also create piplines for doing all steps.\n",
    "\n",
    "Generate output column names for the columns we use the transformers on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf87b359-3d7d-4b11-8dc6-66404e470945",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_cols_ohe = [col + '_OHE' for col in  cat_cols]\n",
    "num_cols_out = [col + '_SCALED' for col in num_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2950791b-aba0-4499-abf3-25dd6ae4adb3",
   "metadata": {},
   "source": [
    "We want to use 1 and 0 for the target column (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33251a96-67e4-4649-a0ea-1f039418f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_marketing_binned_prep = df_bank_marketing_binned_prep.with_column(\"Y\", F.iff(F.col(\"Y\") == F.lit(\"yes\"), F.lit(1), F.lit(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fba8c9-7405-4d5b-9d1a-16300f9e77d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_schema(df_bank_marketing_binned_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c9f2c1-4133-48e9-ae86-ebcd0f6a3f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank_marketing_binned_prep.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b937178-d0aa-4e6d-ad2a-5bf1c8a93793",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "                                # Standard scaler for numerical columns\n",
    "preprocessor = Pipeline(steps=[ ('scaler', pp.StandardScaler(input_cols=num_cols, output_cols=num_cols_out, drop_input_cols=True))\n",
    "                               # One Hot Encoder transformer for categorical columns\n",
    "                               , ('onehot', pp.OneHotEncoder(input_cols=cat_cols, output_cols=cat_cols_ohe, drop_input_cols=True, sparse=False, handle_unknown='ignore'))])\n",
    "\n",
    "# Combine into one pipline with a RandomForestClassifier\n",
    "model_pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', RandomForestClassifier(label_cols=target_col, output_cols=['PREDICTED_RESPONSE'], n_jobs=-1))])\n",
    "\n",
    "model_pipe.fit(df_bank_marketing_binned_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b44c2d-6904-4e44-a3e4-76b2f077a519",
   "metadata": {},
   "source": [
    "Get a Snowpark DataFrame with the predictions using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b20811-8e04-4a8a-ab3a-0a6a8a136e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = model_pipe.predict(df_bank_marketing_binned_prep).cache_result()\n",
    "df_predictions.show()\n",
    "\n",
    "skl = model_pipe.to_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2983f63c-9cc5-4885-9716-65e33b544ad9",
   "metadata": {},
   "source": [
    "Calculate metrics based on the training data, we will use those later when stroing the pipeline into the model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5565008-677b-43dd-805e-652346ef0aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_fscore_metrics = precision_recall_fscore_support(df=df_predictions, y_true_col_names='Y', y_pred_col_names='PREDICTED_RESPONSE', average='binary')\n",
    "accuracy_metric =  accuracy_score(df=df_predictions, y_true_col_names='Y', y_pred_col_names='PREDICTED_RESPONSE')\n",
    "cm = confusion_matrix(df=df_predictions, y_true_col_name='Y', y_pred_col_name='PREDICTED_RESPONSE')\n",
    "print(f\"Precision: {precision_recall_fscore_metrics[0]}\")\n",
    "print(f\"Recall: {precision_recall_fscore_metrics[1]}\")\n",
    "print(f\"fbeta: {precision_recall_fscore_metrics[2]}\")\n",
    "print(f\"Accuracy: {accuracy_metric}\")\n",
    "print(f\"Confusion matrix: {cm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f26f734-fa41-40a2-9d34-b9358673ec44",
   "metadata": {},
   "source": [
    "## Deploy Model/Pipeline using Snowflake Model Registry (Private Preview)\n",
    "The Snowflake Model Registry allows us to store models/piplines in Snowflake with additional metadata, it allows us also to deploy those models to Snowflake and to retrive them. The API also can be used to apply a model on data.\n",
    "\n",
    "### Open/Create Model Registry\n",
    "A model registry needs to be created before it can be used. The creation will create a new database in the current account so the active role needs to have permissions to create a database. After the first creation, the model registry can be opened without the need to create it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbeed82-76be-4122-9e1e-f6299bcc8344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model registry. This will be a no-op if the registry already exists.\n",
    "model_registry.create_model_registry(session=session, database_name='MODEL_REGISTRY', schema_name='MODEL_REGISTRY_SCHEMA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0b8676-6663-48f9-a778-2753836fad90",
   "metadata": {},
   "source": [
    "Connect to the model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e06aed4-0ff2-4b55-91ac-eb279b463aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowml_registry = model_registry.ModelRegistry(session=session, database_name='MODEL_REGISTRY', schema_name='MODEL_REGISTRY_SCHEMA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaff709-35ae-4a57-803e-e4188ec7fbf7",
   "metadata": {},
   "source": [
    "### Register a new Model\n",
    "Registering a new model is always performed through the relational API.\n",
    "\n",
    "The call to log_model executes a few steps:\n",
    "\n",
    "1. The given model object is serialized and uploaded to a stage.\n",
    "2. An entry in the Model Registry is created for the model, referencing the model stage location.\n",
    "3. Additional metadata is updated for the model as provided in the call.\n",
    "\n",
    "For the serialization to work, the model object needs to be serializable in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfc9694-9b37-4cfc-b2b2-9d0ba837dddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nm = 'pp_predict_response'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec68fbe-b4c9-4390-affc-6d79c3c312bf",
   "metadata": {},
   "source": [
    "Check if we already have stored a model with the same name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d768c5-e66b-417f-94ee-5cc08b01c087",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = snowml_registry.list_models()\n",
    "model_list.filter(F.col(\"NAME\") == model_nm).select(\"NAME\", \"VERSION\", \"TYPE\", \"TAGS\", \"METRICS\").show(max_width=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa328c18-c893-42ad-85e2-771a78bf4b45",
   "metadata": {},
   "source": [
    "We can store multiple models with the same name as long as they have different versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb387e5b-5bea-48f6-9ab0-92d2117a5b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0664715-42e7-47ae-8072-5adbd8f7076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref = snowml_registry.log_model(model=model_pipe, model_name=model_nm, model_version=model_v,\n",
    "                                     description='4th Version of a Pipline with OneHoteEncoder, StandardScaler and RandomForestClassifier to predict response',\n",
    "                                     tags={\n",
    "                                        \"stage\": \"testing\", \"classifier_type\": \"pipeline\"},\n",
    "                                    options={\"embed_local_ml_library\": True},)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611838f8-a11f-4837-836f-99f1ebd84159",
   "metadata": {},
   "source": [
    "The log_model method will return a reference to the model, we can use it for getting information about the store model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3ae615-261a-4e6d-b25e-83fd9aa14c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref.get_name() , model_ref.get_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c2165a-22e5-4b39-bfa1-c39cddc18868",
   "metadata": {},
   "source": [
    "Check that the model is in the model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50faea7e-e9c1-4935-b296-f32a91f2b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list.filter(F.col(\"NAME\") == model_nm).select(\"NAME\", \"VERSION\", \"TYPE\", \"TAGS\", \"METRICS\").show(max_width=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a470691a-05f0-4819-b389-4b5739d90184",
   "metadata": {},
   "source": [
    "### Add Metrics\n",
    "Metrics are a type of metadata annotation that can be associated with models stored in the Model Registry. Metrics often take the form of scalars but we also support more complex objects such as arrays or dictionaries to represent metrics. In the exmamples below, we add scalars, dictionaries, and a 2-dimensional numpy array as metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae54fe4-88f5-4c60-b1e1-7bb5a0ad78ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add metrics\n",
    "model_ref.set_metric(metric_name=\"train_accuracy\", metric_value=accuracy_metric)\n",
    "model_ref.set_metric(metric_name=\"train_precision\", metric_value=precision_recall_fscore_metrics[0])\n",
    "model_ref.set_metric(metric_name=\"train_recall\", metric_value=precision_recall_fscore_metrics[1])\n",
    "model_ref.set_metric(metric_name=\"train_f1\", metric_value=precision_recall_fscore_metrics[2])\n",
    "model_ref.set_metric(metric_name=\"train_confusion_matrix\", metric_value=cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddc1aad-fa8e-42c6-b76b-7a673b25c2d2",
   "metadata": {},
   "source": [
    "Get all metrics for a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25a8ef1-b1ae-414b-8ad6-61534de4532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c31b0f9-7ae5-48bf-ac76-1131f7d3252a",
   "metadata": {},
   "source": [
    "Get value for one metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e6f397-7341-4742-a051-f4c04df36131",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref.get_metric_value('train_precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f53172-5f61-48c1-9077-1f16c8789058",
   "metadata": {},
   "source": [
    "### List Model in Registry\n",
    "Listing models in the registry returns a SnowPark DataFrame. That allows the caller to select and filter the models as needed. In the example below, we list the name, version, tags, and metrics for the model we just added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5235a1-1176-4c23-8db5-814c58f85228",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = snowml_registry.list_models()\n",
    "model_list.filter(F.col(\"NAME\") == 'pp_predict_response').select(\"NAME\", \"VERSION\", \"TYPE\", \"TAGS\", \"METRICS\").show(max_width=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6ced8a-6874-4d5f-bced-5ea4d45742a1",
   "metadata": {},
   "source": [
    "### Model Deployment\n",
    "Registry can be used to create deployment, which can be used for prediction. Deployment exists in the form of UDF. It could be either permanent or temporary.\n",
    "\n",
    "#### Permanent deployment\n",
    "Start by checking if we already have deployments for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370c3078-2a37-4602-bad2-937e46ab84ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref.list_deployments().select(\"MODEL_NAME\", \"MODEL_VERSION\", \"DEPLOYMENT_NAME\", \"TARGET_PLATFORM\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21976df-b653-4e20-b5e8-7f35e5319d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_name = f\"pp_predict_response_{model_v}_udf\"\n",
    "model_ref.deploy(deployment_name=deploy_name, target_method='predict', permanent=True, options={\"relax_version\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5a3ae2-8eeb-4f6c-ad93-f48d62c57cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref.list_deployments().select(\"MODEL_NAME\", \"MODEL_VERSION\", \"DEPLOYMENT_NAME\", \"TARGET_PLATFORM\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bb64f5-6c6a-43fc-9b59-976fe144ddd6",
   "metadata": {},
   "source": [
    "Use the deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507c972f-037d-4f27-8f41-07ee6707ec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref.predict(deployment_name=deploy_name, data=df_bank_marketing_binned_prep).select(\"PREDICTED_RESPONSE\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0f1eda-7169-43eb-bdf2-6d144dbfa6c1",
   "metadata": {},
   "source": [
    "Since the model are deployed as a UDF we can call it directly, using SQL or Snowpark. The UDF will expect a ovject (dict) with all columns in it and for generating that we can use the object_construct function. The UDF will also return a object/dict so we need to extract the PREDICTED_RESPONSE value form it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d54cc5c-0de3-4ea4-a858-78c0821d49ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_list = [] \n",
    "for col in df_bank_marketing_binned_prep.columns:\n",
    "    object_list.extend([F.lit(col), F.col(col)])\n",
    "\n",
    "df_bank_marketing_binned_prep.select(F.call_function(\"model_registry.MODEL_REGISTRY_SCHEMA.pp_predict_response_1_udf\", F.object_construct(*object_list)).as_('response')).select(F.col(\"response\")['PREDICTED_RESPONSE'].as_('PREDICTED_RESPONSE')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505e6296-da71-4ce9-9e00-ce3e713193cc",
   "metadata": {},
   "source": [
    "### Load a model from the registry\n",
    "It is also possible to load a store model back into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caf2f0a-7918-43ba-97f6-c2e067d00d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_model = model_ref.load_model()\n",
    "restored_model.predict(df_bank_marketing_binned_prep).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f81e8a-686a-4a3b-a8d3-f189f3f94186",
   "metadata": {},
   "source": [
    "### Model Registry with Scikit-Learn\n",
    "\n",
    "Snowflake Model Registry also supports popoular open source Python ML frameworks such as scikit-learn, XGBoost, HuggingFace, PyTorch etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c53c6-7d31-46e6-9617-7c49af3d7445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as skl_RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder as skl_OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler as skl_StandardScaler\n",
    "from sklearn.compose import ColumnTransformer as skl_ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline as skl_Pipeline\n",
    "\n",
    "pd_train = df_bank_marketing_binned_prep.to_pandas()\n",
    "X = pd_train[[*cat_cols, *num_cols]]\n",
    "y = pd_train[\"Y\"]\n",
    "\n",
    "# One Hot Encoder transformer for categorical columns\n",
    "cat_transformer = skl_Pipeline(steps=[\n",
    "    ('onehot', skl_OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "# Standard scaler for numerical columns\n",
    "num_transformer = skl_Pipeline(steps=[\n",
    "    ('scaler', skl_StandardScaler())\n",
    "])\n",
    "\n",
    "# Combine into a column transformer\n",
    "preprocessor = skl_ColumnTransformer(\n",
    "  [\n",
    "        ('num', num_transformer, num_cols),\n",
    "        ('cat', cat_transformer, cat_cols),\n",
    "    ],  verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "# Create a pipeline with the column transformer and training of a Random Forrest Classifier\n",
    "pipe = skl_Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                       ('classifier', skl_RandomForestClassifier(n_jobs=-1))])\n",
    "rfc_model = pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c37a4e5-81c7-4b7c-a503-5d0420217936",
   "metadata": {},
   "outputs": [],
   "source": [
    "skl_model_name = \"skl_predict_response\"\n",
    "skl_model_ref = snowml_registry.log_model(model=rfc_model, model_name=skl_model_name, model_version='1',\n",
    "                                     description='Scikit-Learn Pipline with OneHoteEncoder, StandardScaler and RandomForestClassifier to predict response',\n",
    "                                     tags={\n",
    "                                        \"stage\": \"testing\", \"classifier_type\": \"pipeline\"},sample_input_data=X.head(), options={\"embed_local_ml_library\": True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7925051a-4049-4ec6-9d88-f859135b2352",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list.select(\"NAME\", \"VERSION\", \"TYPE\", \"TAGS\", \"METRICS\").show(max_width=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587fddcc-2e90-4bf8-a372-cf903d439deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "skl_deploy_name = \"skl_model_response_udf\"\n",
    "skl_model_ref.deploy(deployment_name=skl_deploy_name, target_method='predict', permanent=True,options={\"relax_version\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cbbb64-03f4-434a-937a-b5fc7cca20d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "skl_model_ref.predict(deployment_name=skl_deploy_name, data=X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a820c21d-390b-4a9c-a759-134d500b1ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
