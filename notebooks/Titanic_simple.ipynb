{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5507b95-b310-404a-b5ab-1993dd672405",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Snowpark For Python -- Titanic Survival Prediction\n",
    "\n",
    "### In this session, we will cover:\n",
    "\n",
    "* Creating Session object and connecting to Snowflake\n",
    "* Loading data from Snowflake table into Snowpark DataFrame\n",
    "* Creating Stored Procedure to deploy model training code on Snowflake\n",
    "* Creating User-Defined Function (UDF) for inference\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b82c4b-7c3f-41d0-90f0-6aab2bfc343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d784ea79-2ec9-457a-ad36-f36177656593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import types as T\n",
    "from snowflake.snowpark import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae346f3-e55d-4a9c-a3b4-28d67826ff6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as skl\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc102ff3-00f7-4048-89ff-03526c7ce883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlparse\n",
    "import pandas as pd\n",
    "import cachetools\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd2e279-fc81-4ae8-bb1b-9c66f461cf0e",
   "metadata": {},
   "source": [
    "### Connect to Snowflake\n",
    "Create a connection to Snowflake, Snowpark supports the following authentification methods:\n",
    "* Username and password\n",
    "* externalbrowser (Okta, ADFS, or any other SAML 2.0-compliant identity provider (IdP))\n",
    "* oauth\n",
    "* Key pair\n",
    "\n",
    "This example is using a JSON file with the following structure\n",
    "```\n",
    "{\n",
    "    \"account\":\"MY SNOWFLAKE ACCOUNT\",\n",
    "    \"user\": \"MY USER\",\n",
    "    \"password\":\"MY PASSWORD\",\n",
    "    \"role\":\"MY ROLE\",\n",
    "    \"warehouse\":\"MY WH\",\n",
    "    \"database\":\"MY DB\",\n",
    "    \"schema\":\"MY SCHEMA\"\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d19be7-82da-4eb9-ae5f-13766ce5a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../creds.json') as f:\n",
    "    connection_parameters = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2e4780-ccda-41e0-8b6f-749c098ad91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session.builder.configs(connection_parameters).create()\n",
    "session.sql_simplifier_enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f923dc16-fbb4-4a42-8669-d7db6d626a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Current schema: {session.get_fully_qualified_current_schema()}\")\n",
    "print(f\"Current role: {session.get_current_role()}\")\n",
    "print(f\"Current warehouse: {session.get_current_warehouse()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b86767-4a17-49fd-af67-ba7041db50d6",
   "metadata": {},
   "source": [
    "### Create a DataFrame based on the Titanic table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec409099-643c-4a6f-8095-f6d48027ecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = session.table(\"titanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1ec910-1c53-4c38-94b8-3e8bc8f8ae7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Number of rows: {titanic_df.count()}\")\n",
    "titanic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f14993-abc4-4612-ae5b-f2d6588e1e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f2130-a22e-48a4-85a3-70c6a6c73ee8",
   "metadata": {},
   "source": [
    "### Basic analysis\n",
    "\n",
    "\n",
    "Count by Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02397f59-693c-4da2-97e1-5b0b04de12aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.group_by(\"SURVIVED\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ed508d-0051-4fa9-8ce2-6ddf9c915a44",
   "metadata": {},
   "source": [
    "Add percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2341c459-f72e-401c-bb03-715378d41212",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_to_report = F.function(\"RATIO_TO_REPORT\")\n",
    "titanic_df.group_by(F.col(\"SURVIVED\")).agg(F.count('*').as_(\"PASSENGERS\"))\\\n",
    "            .select(F.col(\"SURVIVED\"), F.col(\"PASSENGERS\"), (ratio_to_report(F.col(\"PASSENGERS\")).over() * 100).as_(\"percentage\") )\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50608fd-9fbc-452c-87ae-e899a4fef1e5",
   "metadata": {},
   "source": [
    "Describe all numeric and categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6539bf77-1a40-4e90-bb2c-f7acfb0b3b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d4b354-4520-47e6-9b33-2413a0799da9",
   "metadata": {},
   "source": [
    "Based on above statistics can drop some of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6885c2b6-b477-4a8f-b3d4-a77e9890375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.drop([\"NAME\", \"TICKET\", \"CABIN\", \"BOAT\", \"BODY\", \"HOME_DEST\", \"SIBSP\", \"PARCH\"])\n",
    "titanic_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a8a545-7a6a-4df7-86c6-f44438832bc8",
   "metadata": {},
   "source": [
    "Let's have a look at the datatypes for the remaining colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0511ac33-ee68-49f0-bd3c-9afcca8b1920",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in titanic_df.schema.fields:\n",
    "    print(f\"{col.name}, Nullable: {col.nullable}, {col.datatype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2d41b7-ac0d-4e2e-b5aa-164ee217266f",
   "metadata": {},
   "source": [
    "PCLASS is stored as a number but is a categorical variable so we can change it character instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178313e7-c6a0-4bf1-b1ea-2894e3797baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.with_column(\"PCLASS\", F.to_varchar(\"PCLASS\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7977b5-0b71-4095-b8bd-91081bdb45d5",
   "metadata": {},
   "source": [
    "It also seems like there is null values in EMBARKED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be18a22d-d9ed-49fe-853b-9cd98f1de3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.group_by(\"EMBARKED\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7a2a83-03a4-44b4-b574-19f4a403cd0d",
   "metadata": {},
   "source": [
    "Replace missing values in EMARKED with S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d29a35-2417-49e4-be70-7b19c590dcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.fillna({\"EMBARKED\": \"S\"})\n",
    "titanic_df.group_by(\"EMBARKED\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f199ab-8c6c-4a62-bc82-0e9c5524a265",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sqlparse.format(titanic_df.queries['queries'][0], reindent=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14967e04-8cab-493d-8bad-ce74302c18fe",
   "metadata": {},
   "source": [
    "Next letâ€™s look at the relationship between each of the features and our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420dce82-4e3b-4e50-903d-9b91e1c95363",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [c.name for c in titanic_df.schema.fields if type(c.datatype) == T.StringType]\n",
    "for col in cols:\n",
    "    if col != \"SURVIVED\":\n",
    "        window = Window.partition_by(col)\n",
    "        display(titanic_df.group_by(col, \"SURVIVED\").count().select(col, \"SURVIVED\", (ratio_to_report(F.col(\"COUNT\")).over(window) * 100).as_(\"percentage\") ).pivot(\"SURVIVED\", ['0', '1']).agg(F.sum(\"percentage\")).show(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd085404-14a3-4b65-9421-8246fe14ffad",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "We are going to create a function for training a model that we will run in Snowflake as a Stored Procedure.\n",
    "\n",
    "Start with selecting the columns we are going to use and pull the data back as a Pandas dataframe so we can test the function locally, if we had have lota of data we would have taken a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1531e4-103f-4a18-bf14-a02e5f09d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = titanic_df.sample(frac=0.10).to_pandas()\n",
    "\n",
    "X = df[[\"EMBARKED\", \"SEX\", \"PCLASS\", \"AGE\", \"FARE\"]]\n",
    "y = df[\"SURVIVED\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d791a8a7-89b0-4854-81a4-2f61ce718a86",
   "metadata": {},
   "source": [
    "Define the training function where we will also do some data preprocessing, by using Pipeline we can then reuse the processing when using the model for prediictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10782a7e-9ee7-4000-9bb6-dd4c145bc04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y):\n",
    "    \n",
    "    # Imputer and transformer for categorical columns. Even if we handled missing values in training data we can not be sure it will happen in production.\n",
    "    cat_cols = [\"EMBARKED\", \"SEX\", \"PCLASS\"]\n",
    "    cat_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "    ])\n",
    "    # Imputer and Scaler for numerical columns\n",
    "    num_cols = [\"AGE\", \"FARE\"]\n",
    "    num_transformer = Pipeline(steps=[\n",
    "        ('imputer', KNNImputer(n_neighbors=5)),\n",
    "        ('scaler', RobustScaler())\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer(\n",
    "      [\n",
    "            ('num', num_transformer, num_cols),\n",
    "            ('cat', cat_transformer, cat_cols)\n",
    "        ],  verbose_feature_names_out=False,\n",
    "    )\n",
    "    \n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', RandomForestClassifier())])\n",
    "    model = pipe.fit(X, y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d35096f-7428-4f9a-95f7-dcbe465df9b8",
   "metadata": {},
   "source": [
    "Test the function locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21a68dd-3440-42c4-a771-9f77ea3c27d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5debf337-4780-44cc-8020-ce0c23b5448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d30f56-d2f7-40d1-a3af-ed165badea15",
   "metadata": {},
   "source": [
    "Check versions of local packages that we are going to use in Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db3398c-5091-4409-ad1c-537894cd6f58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Local Pandas version: {pd.__version__}\")\n",
    "print(f\"Local scikit-learn version: {skl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beb6c05-35a9-4ad3-93b1-8a766d7391b7",
   "metadata": {},
   "source": [
    "Check version avalible in Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1060453-2ebc-44f5-83a0-881d4c69491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.table(\"information_schema.packages\").filter((F.col(\"language\") == 'python') & F.col(\"PACKAGE_NAME\").in_(['pandas', 'scikit-learn']))\\\n",
    "        .sort(F.col(\"PACKAGE_NAME\").asc(), F.col(\"VERSION\").desc()).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5f01a1-535f-4642-9d95-e42cb293c415",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_version = 'pandas==1.4.4'\n",
    "sklearn_version = 'scikit-learn==1.1.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f035acf5-bb43-45eb-b4b1-7de51cbe208e",
   "metadata": {},
   "source": [
    "Helper function to create a Snowflake internal stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14f8618-6289-4930-89ea-b636c6bffda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stage(snf_session: Session, stage_name: str):\n",
    "    return snf_session.sql(f\"create or replace stage {stage_name}\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d63b02-02db-4212-8fd1-051061d2830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_stage(session, \"sp_stage\")\n",
    "create_stage(session, \"model_stage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b778159c-6aee-4651-8d3c-e7b0d5445475",
   "metadata": {},
   "source": [
    "Helper function used to save a object to a Snowflake stage, used within the training Stored procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26491050-24f0-44f3-9e72-ed867fd93331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file_to_stage(snf_session, obj, file_name, stage_path):\n",
    "    import io\n",
    "    import joblib\n",
    "    \n",
    "    file_path = stage_path + file_name\n",
    "    \n",
    "    input_stream = io.BytesIO()\n",
    "    joblib.dump(obj, input_stream)\n",
    "    snf_session._conn._cursor.upload_stream(input_stream, file_path)\n",
    "    \n",
    "    return file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c43c1f-78e1-4907-baa4-f29ab042fb38",
   "metadata": {},
   "source": [
    "Primary function for the Python Stored Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166aafdd-2d53-4f66-8374-9a7fcfc2fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_titanic(snf_session: Session, stage: str) -> dict:\n",
    "    from datetime import datetime\n",
    "    \n",
    "    df_titanic = snf_session.table(\"titanic\").select(\"EMBARKED\", \"SEX\", \"PCLASS\", \"AGE\", \"FARE\", \"SURVIVED\")\n",
    "    df_train, df_test = df_titanic.random_split([0.8, 0.2])\n",
    "    \n",
    "    pd_train = df_train.to_pandas()\n",
    "    \n",
    "    X = pd_train[[\"EMBARKED\", \"SEX\", \"PCLASS\", \"AGE\", \"FARE\"]]\n",
    "    y = pd_train[\"SURVIVED\"]\n",
    "    \n",
    "    # fit the pipeline\n",
    "    model = train(X, y)\n",
    "    \n",
    "    # Test the model\n",
    "    pd_test = df_test.to_pandas()\n",
    "    \n",
    "    X_test = pd_test[[\"EMBARKED\", \"SEX\", \"PCLASS\", \"AGE\", \"FARE\"]]\n",
    "    y_test = pd_test[\"SURVIVED\"]\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Create a dict with some test scores based on test data to return\n",
    "    ret_dict = {\"f1_score\" : f1_score(y_test, y_pred, average='macro')\n",
    "                  , \"precision_score\": precision_score(y_test, y_pred, average='macro')\n",
    "                  , \"recall_score\": recall_score(y_test, y_pred, average='macro')\n",
    "                  , \"accuracy_score\" : accuracy_score(y_test, y_pred)}\n",
    "\n",
    "    now = datetime.now()\n",
    "    # Save the model to stage\n",
    "    save_path = now.strftime(\"%Y-%m-%d-%H%M%S\")\n",
    "    model_path = save_file_to_stage(snf_session, model, 'rfc_survive_model.joblib', f'@{stage}/{save_path}/')\n",
    "    ret_dict['model_path'] = model_path\n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c0ef2e-bece-44f4-b7b9-07e98e81d9ef",
   "metadata": {},
   "source": [
    "Create the Store Procedure in Snowflake.\n",
    "The **sproc** function returns a callable object that can be used to call the stored procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bd4460-c195-475f-b855-b3867df1beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.clear_imports()\n",
    "session.clear_packages()\n",
    "session.add_packages('snowflake-snowpark-python',pandas_version, sklearn_version, 'cloudpickle==2.0.0', 'joblib')\n",
    "train_titanic_sp = F.sproc(func=train_titanic,name=\"train_titanic\", is_permanent = True, replace= True, stage_location = 'sp_stage/titanic/sp/', session=session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1918c26-9d27-450e-8063-402c3eb6f283",
   "metadata": {},
   "source": [
    "Run the training Stored Procedure in Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6208ec6-dddd-4e6c-ba5e-71ac6f602efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_vals = json.loads(train_titanic_sp(session, 'model_stage/titanic'))\n",
    "ret_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1389dd-9785-408f-80be-5b9daa1f1b06",
   "metadata": {},
   "source": [
    "Verify that the model is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50ddbea-fbb1-487e-85a6-b9b17aad3644",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(\"ls @model_stage\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8956c-bf56-4445-b3c4-3db0a6f30423",
   "metadata": {},
   "source": [
    "Get the model file name and path to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f68f3a-c281-42fd-9aba-7045e7b2515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = ret_vals['model_path'].split('/')[-1]\n",
    "stage_path = ret_vals['model_path'][:-(len(model_name)+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d362459-515e-46b4-bf5a-de37fad9a519",
   "metadata": {},
   "source": [
    "Deploy model as a UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f43e642-383b-4bff-94c7-464b4b3e9248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model(snf_session, udf_name, udf_stage, model_name, model_path):\n",
    "    \n",
    "    import_model_path = model_path + '/' + model_name\n",
    "    # Function to load the model file, using cachetools makes sure file is only loaded once\n",
    "    @cachetools.cached(cache={})\n",
    "    def read_file(filename):\n",
    "        import joblib\n",
    "        import sys\n",
    "        import os\n",
    "\n",
    "        import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n",
    "        if import_dir:\n",
    "            with open(os.path.join(import_dir, filename), 'rb') as file:\n",
    "                m = joblib.load(file)\n",
    "                return m\n",
    "    \n",
    "    # Use a vectorized udf, gets maximum 100 rows at the time\n",
    "    @F.udf(name = udf_name, max_batch_size=100, is_permanent = True, stage_location = udf_stage, imports = [import_model_path]\n",
    "           , packages = [pandas_version, sklearn_version, 'cachetools'], replace = True, session = snf_session)\n",
    "    def survived(ds: T.PandasSeries[dict]) -> T.PandasSeries[int]:\n",
    "        # Make sure we have the columns in the expected order in the Pandas Dataframe\n",
    "        model = read_file(model_name)\n",
    "        df = pd.io.json.json_normalize(ds)[[\"EMBARKED\", \"SEX\", \"PCLASS\", \"AGE\", \"FARE\"]]\n",
    "        prediction = model.predict(df)\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32097820-425f-45d9-beca-2e1bfa8cb39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_model(session, \"predict_survival\", \"@sp_stage/titanic/udf/\", model_name ,stage_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146f056a-0378-4e92-b0b3-b0bada81217b",
   "metadata": {},
   "source": [
    "Test the deployed mode (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a477bd-3a2a-42d6-863f-597f7bc383e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.select(F.call_function(\"predict_survival\", F.object_construct('*')).as_(\"predicted\"), F.col(\"SURVIVED\").as_(\"actual\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752053fa-5174-4cb6-937f-bb9e9a344098",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cdde3f-f7b1-48c6-b293-3ce598c375ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
