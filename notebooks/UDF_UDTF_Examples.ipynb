{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2d0f473-3e84-410e-b9f8-d8cc86dd946c",
   "metadata": {},
   "source": [
    "# UDF/UDTF Examples\n",
    "\n",
    "This notebook contains diffrenet examples of how to create UDF/UDTF using the Snowpark API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a241ea1-cfd9-41b1-bf59-ee17aad33860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make sure we do not get line breaks when doing show on wide dataframes\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "\n",
    "# Snowpark imports \n",
    "import snowflake.snowpark as S\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import types as T\n",
    "from snowflake.snowpark import Window\n",
    "\n",
    "# Used for reading creds.json\n",
    "import json\n",
    "\n",
    "# Used for UDF examples\n",
    "import cachetools\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Used for UDF/UDTF examples\n",
    "import joblib\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Used for the UDTF examples\n",
    "from collections import Counter\n",
    "from typing import Iterable, Tuple\n",
    "\n",
    "# Print the version of Snowpark we are using\n",
    "print(f\"Using Snowpark: {S.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d3717b-7e4a-4838-9da7-0e12bdc6a4c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93c4ca02-2385-4ad2-945b-93977048509f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Connect to Snowflake\n",
    "\n",
    "This example is using a JSON file with the following structure\n",
    "```\n",
    "{\n",
    "    \"account\":\"MY SNOWFLAKE ACCOUNT\",\n",
    "    \"user\": \"MY USER\",\n",
    "    \"password\":\"MY PASSWORD\",\n",
    "    \"role\":\"MY ROLE\",\n",
    "    \"warehouse\":\"MY WH\",\n",
    "    \"database\":\"MY DB\",\n",
    "    \"schema\":\"MY SCHEMA\"\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b255d52c-2f3f-4978-a308-f1433e8f37b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('../creds.json') as f:\n",
    "    connection_parameters = json.load(f)\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "print(\"Current role: \" + session.get_current_role() + \", Current schema: \" + session.get_fully_qualified_current_schema() + \", Current WH: \" + session.get_current_warehouse())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9f9855-893b-4a0d-af5b-a1f156e86d42",
   "metadata": {},
   "source": [
    "# User Defined Functions (UDF)\n",
    "\n",
    "There is two diffrent types of UDFs :\n",
    "* UDF (Scalar User Defined Function)\n",
    "    * Is a scalar function that returns one output row for each input row. \n",
    "    * The returned row consists of a single column/value.\n",
    "    * Python UDF batch API enables defining UDFs that receive batches of input rows as Pandas DataFrames and return batches of results as Pandas arrays or Series\n",
    "* UDTF (User Defined Tabular Function)\n",
    "    * A tabular function, also called a table function, returns zero, one, or multiple rows for each input row.\n",
    "\n",
    "## UDF\n",
    "A UDF can be created using the **@udf** decorator, the **udf** function or the **udf.register** method ofthe session object. It can be permanent or temporary.\n",
    "\n",
    "Start by creating a UDF that returns a string, by setting *is_permanent=False* the UDF will only be avalible for our user and also only until the active Snowflake session is closed. The Function will be called for each input row ie it is not using the Batch API. By using **session.clear_imports()** and **session.clear_packages()** we make sure that old imports and packages are not included for the creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9288e1-9f74-4208-a1b6-75e886761042",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.clear_imports()\n",
    "session.clear_packages()\n",
    "@F.udf(name=\"hello_udf\", is_permanent=False, replace=True, session=session)\n",
    "def hello_udf(name: str) -> str:\n",
    "    return f'Hello {name}!'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3851e2f3-ea13-42f3-8a46-5d19e4340e32",
   "metadata": {},
   "source": [
    "Create a DataFrame and test the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16477727-2e4e-4ed6-9559-179958ca0288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_name_df = session.create_dataframe([['Mats'], ['Pia']], schema=[\"name\"])\n",
    "test_name_df.select(F.call_function(\"hello_udf\", F.col(\"name\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73f0595-9c48-462d-aee5-83169ae3698f",
   "metadata": {},
   "source": [
    "A NULL value can be provided to a UDF, it will be converted into a *None* value for the Python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde03c58-e99b-45ce-ba10-cb48fb9864d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_name_with_null_df = session.create_dataframe([['Mats'],[None], ['Pia']], schema=[\"name\"])\n",
    "test_name_with_null_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedc061a-8ae5-425a-b479-83e5bca27f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_name_with_null_df.select(F.call_function(\"hello_udf\", F.col(\"name\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545468f0-c5a4-457d-816b-0ee6d1c4d3b9",
   "metadata": {},
   "source": [
    "Create the same function again using the Python UDF batch API, this is done by changing the parameter to **PandasDataframe** or **PandasSeries** and the return to **PandasSeries**. The benfit of using the Python UDF batch API is that the function will not be called for each input row , but for a batches of rows instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de22105e-725a-4087-aab1-db1ce80e8591",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.clear_imports()\n",
    "session.clear_packages()\n",
    "@F.udf(name=\"hello_batch_udf\", is_permanent=False, replace=True, session=session)\n",
    "def hello_batch_udf(ds: T.PandasSeries[str]) -> T.PandasSeries[str]:\n",
    "    n = len(ds)\n",
    "    return ds.apply(lambda x: f'Hello {x}, we got {n} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630310d2-8567-4ab9-a50c-b8046bc1500f",
   "metadata": {},
   "source": [
    "Use a larger dataset for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b68c42-b2a7-473a-b4bf-0ed890e69598",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customers_df = session.table(\"snowflake_sample_data.tpcds_sf100tcl.customer\")\n",
    "print(f\"Nbr of customers: {customers_df.count():,}\")\n",
    "customers_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f19ad88-2fc1-4689-b79c-fc677bcfffd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "If we test this using **show** we will see that it is not providing 1,000 rows, but 15 since that is the limit we are setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda5c5b8-5be5-48b6-bb6c-8c11d6bc7ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customers_df.select(F.col(\"C_FIRST_NAME\")).select(F.call_function(\"hello_batch_udf\", F.col(\"C_FIRST_NAME\"))).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb0ac85-92ad-407b-a3d0-382522d95975",
   "metadata": {
    "tags": []
   },
   "source": [
    "By using **cache_result** we can temprary store the result of the query generated by the DataFrame and then seee that each call to the function does provide more rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f4a916-3f51-4780-abc0-e21dc14bd395",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_udf_df = customers_df.select(F.call_function(\"hello_batch_udf\", F.col(\"C_FIRST_NAME\"))).cache_result()\n",
    "batch_udf_df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70de83-de0c-42b6-ba85-81f100cf4055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6723be4a-be7e-4d07-b787-52bef1ecf5d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reading files with UDFs\n",
    "\n",
    "To read a file in a UDF, the file needs to be added using **add_import**, the file can either be local or on a Snowflake Stage.\n",
    "\n",
    "If we do not need to update the file, we can refeer to a local file and that file will be uploaded to Snowflake when the UDF is created. If we need to use a newer version of the file we would need to recreate the UDF.\n",
    "\n",
    "By using cachetools we can make sure that the file is only loaded once, since cachetools will cache the return object of the function in memory and return it if the paramtere used in the call is the same.\n",
    "\n",
    "Start by setting where the local files are and the name of the stage we will create later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f435e56d-8b06-4fc3-a0d9-e0126569539f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = \"../data/\"\n",
    "udf_stage_name = \"UDF_DEMO_STAGE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ec8876-180b-4395-a4d9-fde41bfae504",
   "metadata": {},
   "source": [
    "Function to read a file from a stage that a UDF has access to, ie the file needs to be added using the imports parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c77f34-36e7-4739-8ae7-5bbae6647011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@cachetools.cached(cache={})\n",
    "def read_file_cached(filename):\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n",
    "    if import_dir:\n",
    "        with open(os.path.join(import_dir, filename), \"r\") as f:\n",
    "            return f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d240bd8-532d-48d0-b486-2c889ad7387a",
   "metadata": {},
   "source": [
    "Create a UDF where the imports parameter is referring the local file, since we are using the **cachetools** library we also need to add that to the *packages* parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011d4978-c961-4e7f-8b84-cbcd3babeb73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.clear_imports()\n",
    "session.clear_packages()\n",
    "\n",
    "@F.udf(name=\"read_file_static_udf\", is_permanent=False, replace=True, packages=[\"cachetools\"], imports=[f\"{data_path}/text_file.txt\"] ,session=session)\n",
    "def read_file_static() -> str:\n",
    "    return read_file_cached('text_file.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58690d7-45aa-41af-bdb9-2bf57faca7a1",
   "metadata": {},
   "source": [
    "Test the function, since it does not require a input value we can use the **generator** method to generate a DataFrame with one row that has the the result of the call to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74da6b5-59aa-4907-863b-9ab3204945bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.generator(F.call_function(\"read_file_static_udf\"), rowcount=1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612bb5e8-a232-434c-aa09-2bb31bcfb576",
   "metadata": {},
   "source": [
    "If we want to be able to update the file without recreating the UDF, we need to store it in a Snowflake stage, the stage can be either internal or external.\n",
    "\n",
    "Create a Internal Snowflake stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a6484-e93c-422b-87d8-434b82e1c8b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.sql(f\"create or replace stage {udf_stage_name}\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41063811-0dee-4e9d-b11a-71518efd0e6e",
   "metadata": {},
   "source": [
    "Upload a local file to the new stage. If it is a external stage you need to use the tools for it by the Cloud provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37180ab-bbfb-4200-96ed-366221f98839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.file.put(f\"{data_path}text_file.txt\", f\"@{udf_stage_name}\", auto_compress=False, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6224147e-90a9-490e-aac2-618720d3095c",
   "metadata": {},
   "source": [
    "Check that the file is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec498984-47c4-49a5-96b5-ffca6be4c617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.sql(f\"ls @{udf_stage_name}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22641a9-fa7f-4d37-ac0f-71e582ff41ce",
   "metadata": {},
   "source": [
    "Create a UDF that has access to the file in the stage, using the *imports* parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb2fc74-8079-4aee-87c3-1bb25429750b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.clear_imports()\n",
    "session.clear_packages()\n",
    "\n",
    "@F.udf(name=\"read_file_stage_udf\", is_permanent=False, replace=True, packages=[\"cachetools\"], imports=[f\"@{udf_stage_name}/text_file.txt\"] ,session=session)\n",
    "def read_file_stage() -> str:\n",
    "    return read_file_cached('text_file.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7341848e-96c3-4af5-a2a1-41136ac05f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.generator(F.call_function(\"read_file_stage_udf\"), rowcount=1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3016f547-18f1-4a12-8a86-b17402757ba8",
   "metadata": {},
   "source": [
    "If we change the text_file.txt (in the data folder) and upload it it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f140c637-0f3e-435b-988a-ef82a16d8fcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.file.put(f\"{data_path}text_file.txt\", f\"@{udf_stage_name}\", auto_compress=False, overwrite=True)\n",
    "session.sql(f\"ls @{udf_stage_name}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fa29e1-dcae-4f49-bef5-68e2513cfe5e",
   "metadata": {},
   "source": [
    "Rerun the call to the UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8689bd3d-9ed6-43d4-9c06-d82c33bcf817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.generator(F.call_function(\"read_file_stage_udf\"), rowcount=1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96a2c44-ea04-47ea-9b6b-e914a3f2f01c",
   "metadata": {},
   "source": [
    "Creating a UDF that uses as saved Python object. In this case a fitted scikit-learn pipline.\n",
    "\n",
    "Create and fit a pipeline, using titanic data (use **00_Load_demo_data.ipynb** to load the data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f9e25a-c377-4727-b394-4ebdfa5a62fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_cols = [\"EMBARKED\", \"SEX\", \"PCLASS\"]\n",
    "num_cols = [\"AGE\", \"FARE\"]\n",
    "\n",
    "train_df = session.table(\"titanic\").select(*cat_cols, *num_cols, \"SURVIVED\")\n",
    "\n",
    "train_pd = train_df.to_pandas()\n",
    "\n",
    "X = train_pd[[*cat_cols, *num_cols]]\n",
    "y = train_pd[\"SURVIVED\"]\n",
    "\n",
    "# Imputer and OneHotEncoder for categorical columns\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "# Imputer and Scaler for numerical columns\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=5)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "  [\n",
    "        ('num', num_transformer, num_cols),\n",
    "        ('cat', cat_transformer, cat_cols)\n",
    "    ],  verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                       ('classifier', RandomForestClassifier())])\n",
    "\n",
    "rc_pipeline = pipe.fit(X, y)\n",
    "rc_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b1a9cf-a60a-4784-aee2-ddb8034e3591",
   "metadata": {},
   "source": [
    "Save the fitted pipeline as a file locally using joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84937bd7-9e86-4fc6-a241-f40e165e3d64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "joblib.dump(rc_pipeline, \"rc_pipeline.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc064fb-500d-4601-8aa7-03c77659e9c4",
   "metadata": {},
   "source": [
    "Upload the file to the Snowflake stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a87063-29c2-49e6-8d86-4a58b6e54e5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.file.put(\"rc_pipeline.joblib\", f\"@{udf_stage_name}\", auto_compress=False, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b77df23-e914-4b05-b94a-78d85f9ed1d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.sql(f\"ls @{udf_stage_name}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce6962a-4f27-41f8-80c8-17a4ac5204a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Create a function to load the file using joblib, use cachetools so the read from stage is only done once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083c4332-2b75-459c-8562-e4d565f2ffba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@cachetools.cached(cache={})\n",
    "def load_joblib_file(filename):\n",
    "    import joblib\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n",
    "    if import_dir:\n",
    "        with open(os.path.join(import_dir, filename), 'rb') as file:\n",
    "            m = joblib.load(file)\n",
    "            return m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bc65f4-7171-4b6c-8d11-fd0908227517",
   "metadata": {
    "tags": []
   },
   "source": [
    "Create the UDF, it is important that the *imports* parameter is refering the stage and file. Also, only the filename is needed for the *load_joblib_file* function.\n",
    "\n",
    "Since the function is depended on **Pandas**, **scikit-learn** and **cachetools** we need to add them to the *packages* parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ad2cc4-95a6-48fb-b872-0b94e2dc3b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@F.udf(name = \"predict_survive_udf\", is_permanent = False, imports = [f\"@{udf_stage_name}/rc_pipeline.joblib\"]\n",
    "       , packages = ['pandas', 'scikit-learn==1.1.3', 'cachetools'], replace = True, session = session)\n",
    "def predict_survive(pd_df: T.PandasDataFrame[str, str, str, float, float]) -> T.PandasSeries[int]:\n",
    "    \n",
    "    pd_df.columns = [*cat_cols, *num_cols]\n",
    "    model = load_joblib_file('rc_pipeline.joblib') # Only call with the file name!\n",
    "\n",
    "    return model.predict(pd_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aa59a2-8553-414a-ae1f-124ed182638c",
   "metadata": {},
   "source": [
    "Test that the UDF works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa73c0d-5503-416c-b508-226200250feb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_cols = [F.col(col) for col in [*cat_cols, *num_cols]]\n",
    "train_df.with_column(\"PREDICTION\", F.call_function(\"predict_survive_udf\", *input_cols)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7bfbc9-40ab-4975-a82f-acbe09dede50",
   "metadata": {},
   "source": [
    "A batch UDF can also be called by providing the inputs as a array for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7027b99b-919d-4072-bc4c-7d90da9fef8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@F.udf(name = \"predict_survive_array_udf\", is_permanent = False, imports = [f\"@{udf_stage_name}/rc_pipeline.joblib\"]\n",
    "       , packages = ['pandas', 'scikit-learn==1.1.3', 'cachetools'], replace = True, session = session)\n",
    "def predict_survive_array(pd_s: T.PandasSeries[list]) -> T.PandasSeries[int]:\n",
    "    pd_df = pd.DataFrame.from_dict(dict(zip(pd_s.index, pd_s.values))).T\n",
    "    pd_df.columns = [*cat_cols, *num_cols]\n",
    "    model = load_joblib_file('rc_pipeline.joblib') # Only call with the file name!\n",
    "\n",
    "    return model.predict(pd_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32453411-2540-4d25-a11d-eb30be6c567a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.with_column(\"PREDICTION\", F.call_function(\"predict_survive_array_udf\", F.array_construct(*input_cols))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a36d090-f2a2-465b-9fee-e27d76214bbe",
   "metadata": {},
   "source": [
    "Using a dict as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cb487c-5a9c-4f2e-b36b-0b03feb48a5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@F.udf(name = \"predict_survive_dict_udf\", is_permanent = False, imports = [f\"@{udf_stage_name}/rc_pipeline.joblib\"]\n",
    "       , packages = ['pandas', 'scikit-learn==1.1.3', 'cachetools'], replace = True, session = session)\n",
    "def predict_survive_dict(pd_s: T.PandasSeries[dict]) -> T.PandasSeries[int]:\n",
    "    pd_df = pd.io.json.json_normalize(pd_s)[[\"EMBARKED\", \"SEX\", \"PCLASS\", \"AGE\", \"FARE\"]]\n",
    "    model = load_joblib_file('rc_pipeline.joblib') # Only call with the file name!\n",
    "\n",
    "    return model.predict(pd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ec4e68-cc95-4a12-be21-38e640391fd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.with_column(\"PREDICTION\", F.call_function(\"predict_survive_dict_udf\", F.object_construct('*'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd1be76-4884-4989-930a-e02bffcffc03",
   "metadata": {},
   "source": [
    "To return multiple values with a UDF a list or Dict is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c38181-dd5f-42c1-b806-2b2e1c0ef689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@F.udf(name = \"predict_survive_array_return_udf\", is_permanent = False, imports = [f\"@{udf_stage_name}/rc_pipeline.joblib\"]\n",
    "       , packages = ['pandas', 'scikit-learn==1.1.3', 'cachetools'], replace = True, session = session)\n",
    "def predict_survive_array_return(pd_df: T.PandasDataFrame[str, str, str, float, float]) -> T.PandasSeries[list]:\n",
    "    \n",
    "    pd_df.columns = [*cat_cols, *num_cols]\n",
    "    model = load_joblib_file('rc_pipeline.joblib') # Only call with the file name!\n",
    "    prediction_proba = model.predict_proba(pd_df)\n",
    "        \n",
    "    # Get the label for the highest probablility\n",
    "    predicted_classes_idx = np.argmax(prediction_proba, axis=1)\n",
    "    classes = model.classes_\n",
    "    predicted_classes = classes[predicted_classes_idx]\n",
    "\n",
    "    # Create a list with return values\n",
    "    return_array = np.column_stack((prediction_proba, predicted_classes))\n",
    "\n",
    "    return return_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df74586-0f66-4246-98f7-fc5ade5d3d30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.with_column(\"RETURN_ARRAY\", F.call_function(\"predict_survive_array_return_udf\", *input_cols)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f1468-447d-421f-bbd8-eb9170980e48",
   "metadata": {},
   "source": [
    "## UDTF\n",
    "\n",
    "User Defined Table Functions (UDTF) is a function that returns zero, one, or multiple rows for each input row.\n",
    "\n",
    "When creating a UDTF a Python class has to be used as the handler\n",
    "\n",
    "A UDTF handler class implements the following, which Snowflake invokes at run time:\n",
    "* An **__init__** method. Optional. Invoked to initialize stateful processing of input partitions.\n",
    "* A **process** method. Required. Invoked for each input row. The method returns a tabular value as tuples.\n",
    "* An **end_partition** method. Optional. Invoked to finalize processing of input partitions.\n",
    "\n",
    "A UDTF can be created using the **@udtf** decorator, the **udtf** function or the **udtf.register** method ofthe session object. It can be permanent or temporary.\n",
    "\n",
    "Start with a simple UDTF that splits a string into words and fore each unique word it returns a row with it and the number of ocurrances in the string of it. We need to provide the output schema ie the columns of the returning rows. If only names are provided the data types are inheried from the process parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7756d8-7fdc-4470-9ff9-c53530d39354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@F.udtf(name=\"word_count_udtf\", output_schema=[\"word\", \"count\"], is_permanent=False, replace=True, session=session)\n",
    "class MyWordCount:\n",
    "    # Called once for each partition\n",
    "    def __init__(self):\n",
    "        self._total_per_partition = 0\n",
    "    \n",
    "    # Called for each row in a partition\n",
    "    def process(self, s1: str) -> Iterable[Tuple[str, int]]:\n",
    "        words = s1.split()\n",
    "        self._total_per_partition = len(words)\n",
    "        # Counter will return a dict with the uinique words as keys and the number ocurrances as the values\n",
    "        counter = Counter(words) \n",
    "        yield from counter.items()\n",
    "    \n",
    "    # Called after the last row in a partion has been processed\n",
    "    def end_partition(self):\n",
    "        yield (\"partition_total\", self._total_per_partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712f853a-f5cb-4449-9192-b856a880e551",
   "metadata": {},
   "source": [
    "Test the UDTF, by using session.table_function we will get a new DataFrame with the data generated by teh UDTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33476c-a11b-410f-b4e8-685d8fa3e852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_udtf = session.table_function(\"word_count_udtf\", F.lit(\"w1 w2 w2 w3 w3 w3\"))\n",
    "df_udtf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff11d9aa-d7ac-4bc1-afcc-c4bcab82e104",
   "metadata": {},
   "source": [
    "We can also use it with a DataFrame, using call_table_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd36b562-aa76-46b5-b585-8050e576519c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_udtf_data = session.create_dataframe([[\"w1 w2 w2 w3 w3 w3\"]], schema=[\"text\"])\n",
    "df_udtf_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a7572a-028d-479c-84b3-8fe5a1f10025",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_udtf_data.select(F.call_table_function(\"word_count_udtf\", F.col(\"TEXT\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1830123b-d953-4e84-a48c-ec45ffa7d291",
   "metadata": {},
   "source": [
    "If we want to do the split/count by a column, the partition_by parameter can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debeed02-0cde-4462-94f3-ebc4128b8b97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_udtf_part_data = session.create_dataframe([[\"1\", \"w1 w2 w2 w3 w3 w3\"], [\"2\", \"w4 w4 w4 w4 w1\"]], schema=[\"partition\",\"text\"])\n",
    "df_udtf_part_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164ac085-0081-448a-a9ce-6afdc2d76621",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_udtf_part_data.select(\"partition\", F.call_table_function(\"word_count_udtf\", F.col(\"TEXT\")).over(partition_by=\"partition\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dbcbce-de0b-4ae8-bf24-e7cb5ec983a1",
   "metadata": {},
   "source": [
    "Another example of a UDTF that generate a list of previous rows values including current."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03003877-aebd-4d9b-b06a-7daf5b445632",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@F.udtf(name=\"collect_list\", is_permanent=False, replace=True, packages=[\"typing\"], output_schema=T.StructType([T.StructField(\"list\", T.ArrayType())]), session=session)\n",
    "class collect_list_handler:\n",
    "    def __init__(self) -> None:\n",
    "        self.list = []\n",
    "    def process(self, element: float) -> Iterable[Tuple[list]]:\n",
    "        self.list.append(element)\n",
    "        yield (self.list,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e8fd63-6bad-403c-b88f-178f8b9df4cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.with_column(\"collect_list\", F.call_table_function(\"collect_list\", F.col(\"FARE\"))).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf079fb-8081-4139-913a-5c6670f5d57d",
   "metadata": {},
   "source": [
    "We can also use a UDTF for doing Scoring, for example if we want to return multiple columns. Have in mind that this will be row by row execution.\n",
    "\n",
    "The example below uses the sklearn pipline we trained earlier to return the probabilities for 0 and 1 and the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51cfe4f-e098-47be-9cd4-6b50825a8b15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.clear_imports()\n",
    "session.clear_packages()\n",
    "@F.udtf(name=\"predict_survive_udtf\", is_permanent=False, replace=True, packages=['typing', 'pandas', 'numpy', 'joblib', 'scikit-learn==1.1.3'], imports = [f\"@{udf_stage_name}/rc_pipeline.joblib\"]\n",
    "        , output_schema=T.StructType([T.StructField(\"prob_0\", T.FloatType()), T.StructField(\"prob_1\", T.FloatType()), T.StructField(\"prediction\", T.StringType())]), session=session)\n",
    "class predict_survive_handler:\n",
    "    # We load the model from stage at the start of each partition\n",
    "    def __init__(self) -> None:\n",
    "        import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n",
    "        with open(os.path.join(import_dir, 'rc_pipeline.joblib'), 'rb') as file:\n",
    "            self.model = joblib.load(file)\n",
    "        self.classes = self.model.classes_\n",
    "        \n",
    "    # Score each input row\n",
    "    def process(self, embarked: str, sex: str, pclass: str, age: float, fare: float) -> Iterable[Tuple[float, float, str]]:\n",
    "        # Create a Pandas DataFrame of the input values\n",
    "        pd_df = pd.DataFrame([[embarked, sex, pclass, age, fare]], columns=[\"EMBARKED\",\"SEX\", \"PCLASS\", \"AGE\", \"FARE\"])\n",
    "        \n",
    "        # Get the probabilities for 0/1\n",
    "        prediction_proba = self.model.predict_proba(pd_df)[0]\n",
    "        \n",
    "        # Get the label for the highest probablility\n",
    "        predicted_class_idx = np.argmax(prediction_proba)\n",
    "        predicted_class = self.classes[predicted_class_idx]\n",
    "        \n",
    "        # Create a list with return values\n",
    "        return_list = prediction_proba.tolist()\n",
    "        return_list.append(predicted_class)\n",
    "        \n",
    "        # Return the list as a tuple\n",
    "        yield tuple(return_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08be87ec-cf78-4741-82f9-92b63fd71ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = train_df.with_column(\"PCLASS\", F.to_varchar(F.col(\"PCLASS\")))\n",
    "train_df.select( *input_cols, F.call_table_function(\"predict_survive_udtf\", *input_cols)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c8b54-84e3-47b4-a265-7d5efba094a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb982722-2d4c-4d57-87f2-aaae2a90cde1",
   "metadata": {},
   "source": [
    "# Using UDF in other languages\n",
    "\n",
    "A UDF can be created using Python, Java, Scala, JavaScript and SQL and can be used for any of the supported languages.\n",
    "\n",
    "Start by creating a Scala UDF using SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e0a5b9-a26f-4b2d-8e98-adb8c203246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scala_udf_sql = \"\"\"CREATE OR REPLACE FUNCTION scala_double_it(x INTEGER)\n",
    "RETURNS INTEGER\n",
    "LANGUAGE SCALA\n",
    "CALLED ON NULL INPUT\n",
    "RUNTIME_VERSION = 2.12\n",
    "HANDLER='Double.doubleIt'\n",
    "AS\n",
    "$$\n",
    "class Double {\n",
    "  def doubleIt(x : Integer): Integer = {\n",
    "    return 2*x;\n",
    "  }\n",
    "}\n",
    "$$;\n",
    "\"\"\"\n",
    "\n",
    "session.sql(scala_udf_sql).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f228c2-6b38-4d1b-a9c5-c6638e497a0a",
   "metadata": {},
   "source": [
    "Create a Python UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5975b2-8e3d-41c3-bc67-09f295c33701",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(name=\"python_echo_varchar\", is_permanent=False, replace=True, session=session)\n",
    "def python_echo_varchar(ds: T.PandasSeries[str]) -> T.PandasSeries[str]:\n",
    "    return ds.apply(lambda x: f\"Hello {x}, I'm a Python UDF!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa54c6d-5440-4d7c-a34d-6317dfdf5052",
   "metadata": {},
   "source": [
    "Create a DataFrame with testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce19c8fe-db8a-4194-b116-8327dbbe136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_udf_df = session.create_dataframe([['Mats', 2], ['Pia', 3]], schema=[\"name\", \"value\"])\n",
    "test_udf_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36f705a-9f94-490a-8290-e9c11d4131d9",
   "metadata": {},
   "source": [
    "Use both the SCALA and PYTHON UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99702f45-85f9-4657-9dc7-b501191ce88b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_udf_df.select(F.call_function(\"scala_double_it\", F.col(\"value\")).as_(\"SCALA_UDF_RES\"), \n",
    "                   F.call_function(\"python_echo_varchar\", F.col(\"name\")).as_(\"PYTHON_UDF_RES\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa639e44-05d0-4ada-a1d7-15d5b8048136",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0586e688-5218-4d62-9747-e1d5ab8a2696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
