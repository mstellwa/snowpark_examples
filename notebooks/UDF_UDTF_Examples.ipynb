{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2d0f473-3e84-410e-b9f8-d8cc86dd946c",
   "metadata": {},
   "source": [
    "# UDF/UDTF Examples\n",
    "\n",
    "This notebook contains diffrenet examples of how to create UDF/UDTF using the Snowpark API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a241ea1-cfd9-41b1-bf59-ee17aad33860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make sure we do not get line breaks when doing show on wide dataframes\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "\n",
    "# Snowpark imports \n",
    "import snowflake.snowpark as S\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import types as T\n",
    "\n",
    "# Used for UDF examples\n",
    "import cachetools\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Used for UDF/UDTF examples\n",
    "import joblib\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Used for the UDTF examples\n",
    "from collections import Counter\n",
    "from typing import Iterable, Tuple\n",
    "\n",
    "# Print the version of Snowpark we are using\n",
    "print(f\"Using Snowpark: {S.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d3717b-7e4a-4838-9da7-0e12bdc6a4c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93c4ca02-2385-4ad2-945b-93977048509f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Connect to Snowflake\n",
    "\n",
    "This example is using the connections.toml file to connect to Snowflake. You can read more at https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-connect#connecting-using-the-connections-toml-file how to set it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b255d52c-2f3f-4978-a308-f1433e8f37b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONNECTION_NAME = 'mstellwall_aws_us_west3'\n",
    "DATABASE_NAME = 'SNOWPARK_DEMO_DB' # Database that has the source files\n",
    "DATABASE_SCHEMA = 'SOURCE_DATA' # Name of schema that has the source files\n",
    "FULLY_QUALIFIED_NAME = f\"{DATABASE_NAME}.{DATABASE_SCHEMA}\"\n",
    "\n",
    "snf_session = Session.builder.config(\"connection_name\", CONNECTION_NAME).create()\n",
    "snf_session.use_schema(FULLY_QUALIFIED_NAME)\n",
    "print(\"Current role: \" + snf_session.get_current_role() + \", Current schema: \" + snf_session.get_fully_qualified_current_schema() + \", Current WH: \" + snf_session.get_current_warehouse())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9f9855-893b-4a0d-af5b-a1f156e86d42",
   "metadata": {},
   "source": [
    "# User Defined Functions (UDF)\n",
    "\n",
    "There is two diffrent types of UDFs :\n",
    "* UDF (Scalar User Defined Function)\n",
    "    * Is a scalar function that returns one output row for each input row. \n",
    "    * The returned row consists of a single column/value.\n",
    "    * Python UDF batch API enables defining UDFs that receive batches of input rows as Pandas DataFrames and return batches of results as Pandas arrays or Series\n",
    "* UDTF (User Defined Tabular Function)\n",
    "    * A tabular function, also called a table function, returns zero, one, or multiple rows for each input row.\n",
    "\n",
    "## UDF\n",
    "A UDF can be created using the **@udf** decorator, the **udf** function or the **udf.register** method ofthe session object. It can be permanent or temporary.\n",
    "\n",
    "Start by creating a UDF that returns a string, by setting *is_permanent=False* the UDF will only be avalible for our user and also only until the active Snowflake session is closed. The Function will be called for each input row ie it is not using the Batch API. By using **session.clear_imports()** and **session.clear_packages()** we make sure that old imports and packages are not included for the creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9288e1-9f74-4208-a1b6-75e886761042",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snf_session.clear_imports()\n",
    "snf_session.clear_packages()\n",
    "\n",
    "@F.udf(name=\"hello_udf\", is_permanent=False, replace=True, session=snf_session)\n",
    "def hello_udf(name: str) -> str:\n",
    "    return f'Hello {name}!'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3851e2f3-ea13-42f3-8a46-5d19e4340e32",
   "metadata": {},
   "source": [
    "Create a DataFrame and test the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16477727-2e4e-4ed6-9559-179958ca0288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_name_df = snf_session.create_dataframe([['Mats'], ['Pia']], schema=[\"name\"])\n",
    "test_name_df.select(F.call_function(\"hello_udf\", F.col(\"name\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73f0595-9c48-462d-aee5-83169ae3698f",
   "metadata": {},
   "source": [
    "A NULL value can be provided to a UDF, it will be converted into a *None* value for the Python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde03c58-e99b-45ce-ba10-cb48fb9864d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_name_with_null_df = snf_session.create_dataframe([['Mats'],[None], ['Pia']], schema=[\"name\"])\n",
    "test_name_with_null_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedc061a-8ae5-425a-b479-83e5bca27f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_name_with_null_df.select(F.call_function(\"hello_udf\", F.col(\"name\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545468f0-c5a4-457d-816b-0ee6d1c4d3b9",
   "metadata": {},
   "source": [
    "Create the same function again using the Python UDF batch API, this is done by changing the parameter to **PandasDataframe** or **PandasSeries** and the return to **PandasSeries**. The benfit of using the Python UDF batch API is that the function will not be called for each input row , but for a batches of rows instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de22105e-725a-4087-aab1-db1ce80e8591",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@F.udf(name=\"hello_batch_udf\", is_permanent=False, replace=True, session=snf_session)\n",
    "def hello_batch_udf(ds: T.PandasSeries[str]) -> T.PandasSeries[str]:\n",
    "    n = len(ds)\n",
    "    return ds.apply(lambda x: f'Hello {x}, we got {n} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d642a3-454a-42dc-95ea-5697496a61b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name_df.select(F.call_function(\"hello_batch_udf\", F.col(\"name\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630310d2-8567-4ab9-a50c-b8046bc1500f",
   "metadata": {},
   "source": [
    "Use a larger dataset for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b68c42-b2a7-473a-b4bf-0ed890e69598",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customers_df = snf_session.table(\"snowflake_sample_data.tpcds_sf100tcl.customer\")\n",
    "print(f\"Nbr of customers: {customers_df.count():,}\")\n",
    "customers_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f19ad88-2fc1-4689-b79c-fc677bcfffd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "If we test this using **show** we will see that it is only providing 15 rows since that is the limit we are setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda5c5b8-5be5-48b6-bb6c-8c11d6bc7ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customers_df.select(F.col(\"C_FIRST_NAME\")).select(F.call_function(\"hello_batch_udf\", F.col(\"C_FIRST_NAME\"))).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb0ac85-92ad-407b-a3d0-382522d95975",
   "metadata": {
    "tags": []
   },
   "source": [
    "By using **cache_result** we can temprary store the result of the query generated by the DataFrame and then seee that each call to the function does provide more rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f4a916-3f51-4780-abc0-e21dc14bd395",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_udf_df = customers_df.select(F.call_function(\"hello_batch_udf\", F.col(\"C_FIRST_NAME\"))).cache_result()\n",
    "batch_udf_df.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e37088-d430-46ba-a9f6-789f68f0554d",
   "metadata": {},
   "source": [
    "We can also create a UDF based on a Python file, start by creating a directory for storing the files in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f70de83-de0c-42b6-ba85-81f100cf4055",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../py_scripts/udf_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129cb4fd-e5e5-48b2-a2ba-d5518164ad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../py_scripts/udf_examples/udf_from_file.py\n",
    "def hello_udf(name: str) -> str:\n",
    "    return f'Hello {name}!, this is a function in a Python file'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59647b5-e4ea-41bb-bbab-2fefaf8e9feb",
   "metadata": {},
   "source": [
    "Creating a UDF using the file we created, the file we point to will be uploaded to Snowflake during the creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873d39cf-f9e9-40e4-ac6b-a1ae530aeaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_file_udf = snf_session.udf.register_from_file(name=\"udf_local_file\", file_path=\"../py_scripts/udf_examples/udf_from_file.py\", func_name=\"hello_udf\"\n",
    "                                                , replace=True, is_permanent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ace84ed-004b-4f47-b060-f3f029dc62c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name_df.select(F.call_function(\"udf_local_file\", F.col(\"name\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f781fc-0dfc-42f6-8f0e-d81932d0bf89",
   "metadata": {},
   "source": [
    "We can also create a UDF based on a file that is on a Snowflake stage, that enables us to updated the file without having to recreate the UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3376ade-1bbf-40a0-a7af-21fd97f4ce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../py_scripts/udf_examples/udf_from_stage.py\n",
    "def hello_udf(name: str) -> str:\n",
    "    return f'Hello {name}!, this is a function in a Python file that is on a stage'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c5b1b0-e60c-47d9-ab49-2c14ff9c8705",
   "metadata": {},
   "source": [
    "We also need a Snowflake stage to store the file, we can either use a external stage (AWS S3, Azure Blob Storage , Google Cloud Storage) or a internal stage (managed by Snowflake).  In this example we are using a Snowflake internal stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810f5b58-b0e1-49b2-a395-3c9ae2b2910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "snf_session.sql(\"create stage if not exists python_files\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b85b4-4d74-4c62-82f1-4d7dc5c9d249",
   "metadata": {},
   "source": [
    "To ad the file to the stage we can use **file.put** if the stage is a Snowflake Internal, if using a cloud provider we need to use their tools to upload it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecedb3ef-8ad2-40e3-b29f-caf84caf7961",
   "metadata": {},
   "outputs": [],
   "source": [
    "snf_session.file.put('../py_scripts/udf_examples/udf_from_stage.py', '@python_files/udf_examples/', auto_compress=False, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ffe0fd-faf7-4807-acbe-c67783e77880",
   "metadata": {},
   "source": [
    "When creating a Python UDF from a file on a stage we need to provide what data types the arguments and return value have through the **return_type** and **input_types** parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e689e71b-f482-448e-be54-2ba9bab14f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_file_udf = snf_session.udf.register_from_file(name=\"udf_stage_file\", file_path=\"@python_files/udf_examples/udf_from_stage.py\", func_name=\"hello_udf\"\n",
    "                                                , input_types=[T.StringType()], return_type=T.StringType()\n",
    "                                                , replace=True, is_permanent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be759f22-6f0c-4412-a90a-af691bd69755",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name_df.select(F.call_function(\"udf_stage_file\", F.col(\"name\"))).show(max_width=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92030adb-a390-45b3-8c15-57f19287c929",
   "metadata": {},
   "source": [
    "If we update the file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f7eb32-13df-4be3-bffb-93dc018b3305",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../py_scripts/udf_examples/udf_from_stage.py\n",
    "def hello_udf(name: str) -> str:\n",
    "    return f'Hello {name}!, this is a function in a Python file that is on a stage and is now updated'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49313072-3782-438a-9010-6fa36651f0ca",
   "metadata": {},
   "source": [
    "And upload it to our stage, overwriting the existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa8e4ff-d058-403e-bb19-664e45f30a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "snf_session.file.put('../py_scripts/udf_examples/udf_from_stage.py', '@python_files/udf_examples/', auto_compress=False, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c987bef-0b6a-43c4-9570-a52bd606b8cf",
   "metadata": {},
   "source": [
    "And when we now call the UDF we are using the updated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e14788-3134-446f-a981-7b609b0764e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name_df.select(F.call_function(\"udf_stage_file\", F.col(\"name\"))).show(max_width=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6723be4a-be7e-4d07-b787-52bef1ecf5d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reading files with UDFs\n",
    "There is two ways to read files on a stage from a UDxF, either using **add_import** where the file can either be local or on a Snowflake Stage or using **SnowflakeFile** where the files needs to be on a stage that is using the Directory Table.\n",
    "\n",
    "#### Using add_import\n",
    "If we do not need to update the file, we can refeer to a local file and that file will be uploaded to Snowflake when the UDF is created. If we need to use a newer version of the file we would need to recreate the UDF.\n",
    "\n",
    "By using cachetools we can make sure that the file is only loaded once, since cachetools will cache the return object of the function in memory and return it if the paramtere used in the call is the same.\n",
    "\n",
    "Start by setting where the local files are and the name of the stage we will create later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f435e56d-8b06-4fc3-a0d9-e0126569539f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = \"../data/\"\n",
    "udf_stage_name = \"UDF_DEMO_STAGE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5a625e-f675-44ac-a61a-528b789f2bca",
   "metadata": {},
   "source": [
    "Create a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc27b24-b908-4be6-8eec-cb1c292d58cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {data_path}text_file.txt\n",
    "Hello this is the first version!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ec8876-180b-4395-a4d9-fde41bfae504",
   "metadata": {},
   "source": [
    "Function to read a file from a stage that a UDF has access to, ie the file needs to be added using the imports parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c77f34-36e7-4739-8ae7-5bbae6647011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@cachetools.cached(cache={})\n",
    "def read_file_cached(filename):\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n",
    "    if import_dir:\n",
    "        with open(os.path.join(import_dir, filename), \"r\") as f:\n",
    "            return f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d240bd8-532d-48d0-b486-2c889ad7387a",
   "metadata": {},
   "source": [
    "Create a UDF where the imports parameter is referring the local file, since we are using the **cachetools** library we also need to add that to the *packages* parameter. Since we point to the local file, using the **import** parameter, it will uploaded automatically and if we need to change the file we need to re-create the UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011d4978-c961-4e7f-8b84-cbcd3babeb73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@F.udf(name=\"read_file_static_udf\", is_permanent=False, replace=True, packages=[\"cachetools\"], imports=[f\"{data_path}/text_file.txt\"] ,session=snf_session)\n",
    "def read_file_static() -> str:\n",
    "    return read_file_cached('text_file.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58690d7-45aa-41af-bdb9-2bf57faca7a1",
   "metadata": {},
   "source": [
    "Test the function, since it does not require a input value we can use the **generator** method to generate a DataFrame with one row that has the the result of the call to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74da6b5-59aa-4907-863b-9ab3204945bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snf_session.generator(F.call_function(\"read_file_static_udf\"), rowcount=1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612bb5e8-a232-434c-aa09-2bb31bcfb576",
   "metadata": {},
   "source": [
    "If we want to be able to update the file without recreating the UDF, we need to store it in a Snowflake stage, the stage can be either internal or external.\n",
    "\n",
    "Create a Internal Snowflake stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a6484-e93c-422b-87d8-434b82e1c8b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snf_session.sql(f\"create stage if not exists {udf_stage_name}\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41063811-0dee-4e9d-b11a-71518efd0e6e",
   "metadata": {},
   "source": [
    "Upload a local file to the new stage. If it is a external stage you need to use the tools for it by the Cloud provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37180ab-bbfb-4200-96ed-366221f98839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snf_session.file.put(f\"{data_path}text_file.txt\", f\"@{udf_stage_name}\", auto_compress=False, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6224147e-90a9-490e-aac2-618720d3095c",
   "metadata": {},
   "source": [
    "Check that the file is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec498984-47c4-49a5-96b5-ffca6be4c617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snf_session.sql(f\"ls @{udf_stage_name}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22641a9-fa7f-4d37-ac0f-71e582ff41ce",
   "metadata": {},
   "source": [
    "Create a UDF that has access to the file in the stage, using the *imports* parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb2fc74-8079-4aee-87c3-1bb25429750b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@F.udf(name=\"read_file_stage_udf\", is_permanent=False, replace=True, packages=[\"cachetools\"], imports=[f\"@{udf_stage_name}/text_file.txt\"] ,session=snf_session)\n",
    "def read_file_stage() -> str:\n",
    "    return read_file_cached('text_file.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7341848e-96c3-4af5-a2a1-41136ac05f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snf_session.generator(F.call_function(\"read_file_stage_udf\"), rowcount=1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3016f547-18f1-4a12-8a86-b17402757ba8",
   "metadata": {},
   "source": [
    "If we change the text_file.txt (in the data folder) and upload it it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e82795-288a-4b3a-8e14-0f2392d1b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {data_path}text_file.txt\n",
    "Hello this is the second version!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f140c637-0f3e-435b-988a-ef82a16d8fcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snf_session.file.put(f\"{data_path}text_file.txt\", f\"@{udf_stage_name}\", auto_compress=False, overwrite=True)\n",
    "snf_session.sql(f\"ls @{udf_stage_name}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fa29e1-dcae-4f49-bef5-68e2513cfe5e",
   "metadata": {},
   "source": [
    "Rerun the call to the UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8689bd3d-9ed6-43d4-9c06-d82c33bcf817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snf_session.generator(F.call_function(\"read_file_stage_udf\"), rowcount=1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96a2c44-ea04-47ea-9b6b-e914a3f2f01c",
   "metadata": {},
   "source": [
    "Creating a UDF that uses as saved Python object. In this case a fitted scikit-learn pipline.\n",
    "\n",
    "Create and fit a pipeline, using titanic data (use **00_Load_demo_data.ipynb** to load the data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f9e25a-c377-4727-b394-4ebdfa5a62fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_cols = [\"EMBARKED\", \"SEX\", \"PCLASS\"]\n",
    "num_cols = [\"AGE\", \"FARE\"]\n",
    "\n",
    "train_df = snf_session.table(\"titanic\").select(*cat_cols, *num_cols, \"SURVIVED\")\n",
    "\n",
    "train_pd = train_df.to_pandas()\n",
    "\n",
    "X = train_pd[[*cat_cols, *num_cols]]\n",
    "y = train_pd[\"SURVIVED\"]\n",
    "\n",
    "# Imputer and OneHotEncoder for categorical columns\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "# Imputer and Scaler for numerical columns\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=5)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "  [\n",
    "        ('num', num_transformer, num_cols),\n",
    "        ('cat', cat_transformer, cat_cols)\n",
    "    ],  verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                       ('classifier', RandomForestClassifier())])\n",
    "\n",
    "rc_pipeline = pipe.fit(X, y)\n",
    "rc_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b1a9cf-a60a-4784-aee2-ddb8034e3591",
   "metadata": {},
   "source": [
    "Save the fitted pipeline as a file locally using joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84937bd7-9e86-4fc6-a241-f40e165e3d64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "joblib.dump(rc_pipeline, \"rc_pipeline.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc064fb-500d-4601-8aa7-03c77659e9c4",
   "metadata": {},
   "source": [
    "Upload the file to the Snowflake stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a87063-29c2-49e6-8d86-4a58b6e54e5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snf_session.file.put(\"rc_pipeline.joblib\", f\"@{udf_stage_name}\", auto_compress=False, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b77df23-e914-4b05-b94a-78d85f9ed1d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snf_session.sql(f\"ls @{udf_stage_name}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce6962a-4f27-41f8-80c8-17a4ac5204a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Create a function to load the file using joblib, use cachetools so the read from stage is only done once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083c4332-2b75-459c-8562-e4d565f2ffba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@cachetools.cached(cache={})\n",
    "def load_joblib_file(filename):\n",
    "    import joblib\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n",
    "    if import_dir:\n",
    "        with open(os.path.join(import_dir, filename), 'rb') as file:\n",
    "            m = joblib.load(file)\n",
    "            return m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bc65f4-7171-4b6c-8d11-fd0908227517",
   "metadata": {
    "tags": []
   },
   "source": [
    "Create the UDF, it is important that the *imports* parameter is refering the stage and file. Also, only the filename is needed for the *load_joblib_file* function.\n",
    "\n",
    "Since the function is depended on **Pandas**, **scikit-learn** and **cachetools** we need to add them to the *packages* parameter.\n",
    "\n",
    "We will also make sure UDF scikit-learn version matches the local one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f112d2-027a-4457-b16a-0273cc680049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import __version__ as sk_version\n",
    "sk_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f41a0d3-8cba-4e2b-89af-c5e22210aa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import __version__ as pd_version\n",
    "pd_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ad2cc4-95a6-48fb-b872-0b94e2dc3b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@F.udf(name = \"predict_survive_udf\", is_permanent = False, imports = [f\"@{udf_stage_name}/rc_pipeline.joblib\"]\n",
    "       , packages = [f'pandas=={pd_version}', f'scikit-learn=={sk_version}', 'cachetools'], replace = True, session = snf_session)\n",
    "def predict_survive(pd_df: T.PandasDataFrame[str, str, str, float, float]) -> T.PandasSeries[int]:\n",
    "    \n",
    "    pd_df.columns = [*cat_cols, *num_cols]\n",
    "    model = load_joblib_file('rc_pipeline.joblib') # Only call with the file name!\n",
    "\n",
    "    return model.predict(pd_df)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aa59a2-8553-414a-ae1f-124ed182638c",
   "metadata": {},
   "source": [
    "Test that the UDF works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa73c0d-5503-416c-b508-226200250feb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_cols = [F.col(col) for col in [*cat_cols, *num_cols]]\n",
    "train_df.with_column(\"PREDICTION\", F.call_function(\"predict_survive_udf\",  *input_cols)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7bfbc9-40ab-4975-a82f-acbe09dede50",
   "metadata": {},
   "source": [
    "A batch UDF can also be called by providing the inputs as a array for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7027b99b-919d-4072-bc4c-7d90da9fef8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "@F.udf(name = \"predict_survive_array_udf\", is_permanent = False, imports = [f\"@{udf_stage_name}/rc_pipeline.joblib\"]\n",
    "       , packages = [f'pandas=={pd_version}', f'scikit-learn=={sk_version}', 'cachetools'], replace = True, session = snf_session)\n",
    "def predict_survive_array(pd_s: T.PandasSeries[list]) -> T.PandasSeries[int]:\n",
    "    pd_df = pd.DataFrame.from_dict(dict(zip(pd_s.index, pd_s.values))).T\n",
    "    pd_df.columns = [*cat_cols, *num_cols]\n",
    "    model = load_joblib_file('rc_pipeline.joblib') # Only call with the file name!\n",
    "\n",
    "    return model.predict(pd_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32453411-2540-4d25-a11d-eb30be6c567a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.with_column(\"PREDICTION\", F.call_function(\"predict_survive_array_udf\", F.array_construct(*input_cols))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a36d090-f2a2-465b-9fee-e27d76214bbe",
   "metadata": {},
   "source": [
    "In some cases we want to provide a dict as input and that works as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cb487c-5a9c-4f2e-b36b-0b03feb48a5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@F.udf(name = \"predict_survive_dict_udf\", is_permanent = False, imports = [f\"@{udf_stage_name}/rc_pipeline.joblib\"]\n",
    "       , packages = [f'pandas=={pd_version}', f'scikit-learn=={sk_version}', 'cachetools'], replace = True, session = snf_session)\n",
    "def predict_survive_dict(pd_s: T.PandasSeries[dict]) -> T.PandasSeries[int]:\n",
    "    pd_df = pd.json_normalize(pd_s)[[\"EMBARKED\", \"SEX\", \"PCLASS\", \"AGE\", \"FARE\"]]\n",
    "    model = load_joblib_file('rc_pipeline.joblib') # Only call with the file name!\n",
    "\n",
    "    return model.predict(pd_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be034e7a",
   "metadata": {},
   "source": [
    "Using **object_construct** with '*' allows us to create a dict of each row with column_name: column_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ec4e68-cc95-4a12-be21-38e640391fd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.with_column(\"PREDICTION\", F.call_function(\"predict_survive_dict_udf\", F.object_construct('*'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd1be76-4884-4989-930a-e02bffcffc03",
   "metadata": {},
   "source": [
    "To return multiple values from a UDF a list or Dict is needed, below UDF returns a list/array with the probabilities for each class and the prediced class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c38181-dd5f-42c1-b806-2b2e1c0ef689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@F.udf(name = \"predict_survive_array_return_udf\", is_permanent = False, imports = [f\"@{udf_stage_name}/rc_pipeline.joblib\"]\n",
    "       , packages = [f'pandas=={pd_version}', f'scikit-learn=={sk_version}', 'cachetools'], replace = True, session = snf_session)\n",
    "def predict_survive_array_return(pd_df: T.PandasDataFrame[str, str, str, float, float]) -> T.PandasSeries[list]:\n",
    "    \n",
    "    pd_df.columns = [*cat_cols, *num_cols]\n",
    "    model = load_joblib_file('rc_pipeline.joblib') # Only call with the file name!\n",
    "    prediction_proba = model.predict_proba(pd_df)\n",
    "        \n",
    "    # Get the label for the highest probablility\n",
    "    predicted_classes_idx = np.argmax(prediction_proba, axis=1)\n",
    "    classes = model.classes_\n",
    "    predicted_classes = classes[predicted_classes_idx]\n",
    "\n",
    "    # Create a list with return values\n",
    "    return_array = np.column_stack((prediction_proba, predicted_classes))\n",
    "\n",
    "    return return_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df74586-0f66-4246-98f7-fc5ade5d3d30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_cols = [F.col(col) for col in [*cat_cols, *num_cols]]\n",
    "train_df.with_column(\"RETURN_ARRAY\", F.call_function(\"predict_survive_array_return_udf\", *input_cols)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a18fa93",
   "metadata": {},
   "source": [
    "#### Using SnowflakeFile\n",
    "**SnowflakeFile** allows us to read files from a Snowflake stage without using the **add_import**, instead we pass the reference to a file as the input to the UDxF.\n",
    "\n",
    "See end of UDTF section for an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f1468-447d-421f-bbd8-eb9170980e48",
   "metadata": {},
   "source": [
    "## UDTF\n",
    "\n",
    "User Defined Table Functions (UDTF) is a function that returns zero, one, or multiple rows for each input row.\n",
    "\n",
    "When creating a UDTF a Python class has to be used as the handler\n",
    "\n",
    "A UDTF handler class implements the following, which Snowflake invokes at run time:\n",
    "* An **__init__** method. Optional. Invoked to initialize stateful processing of input partitions.\n",
    "* A **process** method. Required. Invoked for each input row. The method returns a tabular value as tuples.\n",
    "* An **end_partition** method. Optional. Invoked to finalize processing of input partitions.\n",
    "\n",
    "A UDTF can be created using the **@udtf** decorator, the **udtf** function or the **udtf.register** method ofthe session object. It can be permanent or temporary.\n",
    "\n",
    "Start with a simple UDTF that splits a string into words and fore each unique word it returns a row with it and the number of ocurrances in the string of it. We need to provide the output schema ie the columns of the returning rows. If only names are provided the data types are inheried from the process parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7756d8-7fdc-4470-9ff9-c53530d39354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@F.udtf(name=\"word_count_udtf\", output_schema=[\"word\", \"count\"], is_permanent=False, replace=True, session=snf_session)\n",
    "class MyWordCount:\n",
    "    # Called once for each partition\n",
    "    def __init__(self):\n",
    "        self._total_per_partition = 0\n",
    "    \n",
    "    # Called for each row in a partition\n",
    "    def process(self, s1: str) -> Iterable[Tuple[str, int]]:\n",
    "        words = s1.split()\n",
    "        self._total_per_partition = len(words)\n",
    "        # Counter will return a dict with the uinique words as keys and the number ocurrances as the values\n",
    "        counter = Counter(words) \n",
    "        yield from counter.items()\n",
    "    \n",
    "    # Called after the last row in a partion has been processed\n",
    "    def end_partition(self):\n",
    "        yield (\"partition_total\", self._total_per_partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712f853a-f5cb-4449-9192-b856a880e551",
   "metadata": {},
   "source": [
    "Test the UDTF, by using session.table_function we will get a new DataFrame with the data generated by teh UDTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33476c-a11b-410f-b4e8-685d8fa3e852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_udtf = snf_session.table_function(\"word_count_udtf\", F.lit(\"w1 w2 w2 w3 w3 w3\"))\n",
    "df_udtf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff11d9aa-d7ac-4bc1-afcc-c4bcab82e104",
   "metadata": {},
   "source": [
    "We can also use it with a DataFrame, using call_table_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd36b562-aa76-46b5-b585-8050e576519c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_udtf_data = snf_session.create_dataframe([[\"w1 w2 w2 w3 w3 w3\"]], schema=[\"text\"])\n",
    "df_udtf_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a7572a-028d-479c-84b3-8fe5a1f10025",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_udtf_data.select(F.call_table_function(\"word_count_udtf\", F.col(\"TEXT\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1830123b-d953-4e84-a48c-ec45ffa7d291",
   "metadata": {},
   "source": [
    "If we want to do the split/count by a column, the partition_by parameter can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debeed02-0cde-4462-94f3-ebc4128b8b97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_udtf_part_data = snf_session.create_dataframe([[\"1\", \"w1 w2 w2 w3 w3 w3\"], [\"2\", \"w4 w4 w4 w4 w1\"]], schema=[\"partition\",\"text\"])\n",
    "df_udtf_part_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164ac085-0081-448a-a9ce-6afdc2d76621",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_udtf_part_data.select(\"partition\", F.call_table_function(\"word_count_udtf\", F.col(\"TEXT\")).over(partition_by=\"partition\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dbcbce-de0b-4ae8-bf24-e7cb5ec983a1",
   "metadata": {},
   "source": [
    "Another example of a UDTF that generate a list of previous rows values including current."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03003877-aebd-4d9b-b06a-7daf5b445632",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@F.udtf(name=\"collect_list\", is_permanent=False, replace=True, packages=[\"typing\"], output_schema=T.StructType([T.StructField(\"list\", T.ArrayType())]), session=snf_session)\n",
    "class collect_list_handler:\n",
    "    def __init__(self) -> None:\n",
    "        self.list = []\n",
    "    def process(self, element: float) -> Iterable[Tuple[list]]:\n",
    "        self.list.append(element)\n",
    "        yield (self.list,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e8fd63-6bad-403c-b88f-178f8b9df4cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.with_column(\"collect_list\", F.call_table_function(\"collect_list\", F.col(\"FARE\"))).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf079fb-8081-4139-913a-5c6670f5d57d",
   "metadata": {},
   "source": [
    "We can also use a UDTF for doing Scoring, for example if we want to return multiple columns. Have in mind that this will be row by row execution.\n",
    "\n",
    "The example below uses the sklearn pipline we trained earlier to return the probabilities for 0 and 1 and the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51cfe4f-e098-47be-9cd4-6b50825a8b15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@F.udtf(name=\"predict_survive_udtf\", is_permanent=False, replace=True, packages=['typing', f'pandas=={pd_version}', 'numpy', 'joblib', f'scikit-learn=={sk_version}']\n",
    "        , imports = [f\"@{udf_stage_name}/rc_pipeline.joblib\"]\n",
    "        , output_schema=T.StructType([T.StructField(\"prob_0\", T.FloatType()), T.StructField(\"prob_1\", T.FloatType()), T.StructField(\"prediction\", T.StringType())]), session=snf_session)\n",
    "class predict_survive_handler:\n",
    "    # We load the model from stage at the start of each partition\n",
    "    def __init__(self) -> None:\n",
    "        import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n",
    "        with open(os.path.join(import_dir, 'rc_pipeline.joblib'), 'rb') as file:\n",
    "            self.model = joblib.load(file)\n",
    "        self.classes = self.model.classes_\n",
    "        \n",
    "    # Score each input row\n",
    "    def process(self, embarked: str, sex: str, pclass: str, age: float, fare: float) -> Iterable[Tuple[float, float, str]]:\n",
    "        # Create a Pandas DataFrame of the input values\n",
    "        pd_df = pd.DataFrame([[embarked, sex, pclass, age, fare]], columns=[\"EMBARKED\",\"SEX\", \"PCLASS\", \"AGE\", \"FARE\"])\n",
    "        \n",
    "        # Get the probabilities for 0/1\n",
    "        prediction_proba = self.model.predict_proba(pd_df)[0]\n",
    "        \n",
    "        # Get the label for the highest probablility\n",
    "        predicted_class_idx = np.argmax(prediction_proba)\n",
    "        predicted_class = self.classes[predicted_class_idx]\n",
    "        \n",
    "        # Create a list with return values\n",
    "        return_list = prediction_proba.tolist()\n",
    "        return_list.append(predicted_class)\n",
    "        \n",
    "        # Return the list as a tuple\n",
    "        yield tuple(return_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08be87ec-cf78-4741-82f9-92b63fd71ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = train_df.with_column(\"PCLASS\", F.to_varchar(F.col(\"PCLASS\")))\n",
    "train_df.select( *input_cols, F.call_table_function(\"predict_survive_udtf\", *input_cols)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f953b403-9b38-4cd7-9899-c2be8481da71",
   "metadata": {},
   "source": [
    "UDTF can also be vectorized, as a UDF, and in the above case where we do not want to process each row individual it makes more sense, and often more preformant, to use a vectroized UDTF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f786a2-dcf2-4792-a02e-18921ff94c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udtf(name=\"predict_survive_batch_2_udtf\", is_permanent=True, replace=True, stage_location=udf_stage_name\n",
    "        , packages=['typing', f'pandas=={pd_version}', 'numpy', 'joblib', f'scikit-learn=={sk_version}']\n",
    "        , imports = [f\"@{udf_stage_name}/rc_pipeline.joblib\"]\n",
    "        , input_types=[T.PandasDataFrameType([T.StringType(), T.StringType(), T.StringType(), T.FloatType(), T.FloatType()])]\n",
    "        , output_schema=T.PandasDataFrameType([T.FloatType(), T.FloatType(), T.StringType()], [\"PROB_0\", \"PROB_1\", \"PREDICTION\"])\n",
    "        #, output_schema=T.PandasDataFrameType([T.StringType()], [\"info\"])\n",
    "        , session=snf_session)\n",
    "class predict_survive_batch_handler:\n",
    "    # We load the model from stage at the start of each partition\n",
    "    def __init__(self) -> None:\n",
    "        import_dir = sys._xoptions.get(\"snowflake_import_directory\")\n",
    "        with open(os.path.join(import_dir, 'rc_pipeline.joblib'), 'rb') as file:\n",
    "            self.model = joblib.load(file)\n",
    "        self.classes = self.model.classes_\n",
    "        \n",
    "    # Score all input rows\n",
    "    def end_partition(self, pd_df):\n",
    "        # Set the column name of the provided Pandas DataFrame\n",
    "        pd_df.columns=[\"EMBARKED\",\"SEX\", \"PCLASS\", \"AGE\", \"FARE\"]\n",
    "        \n",
    "        # Get the probabilities for 0/1\n",
    "        prediction_proba = self.model.predict_proba(pd_df)\n",
    "        \n",
    "        # Get the label for the highest probablility\n",
    "        predicted_class_idx = prediction_proba.argmax(axis=1)\n",
    "        predicted_class = self.classes[predicted_class_idx]\n",
    "        predicted_class = np.expand_dims(predicted_class, axis=0)\n",
    "        \n",
    "        # Create a list with return values\n",
    "        return_list = np.concatenate((prediction_proba, predicted_class.T), axis=1)\n",
    "        \n",
    "        # Return the list as a tuple\n",
    "        yield pd.DataFrame(return_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c68619",
   "metadata": {},
   "source": [
    "Test the vectroized UDTF, the input columns will all have NULL values in the returning DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5938c8-8b5e-43c1-a590-f99e07e18d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = train_df.with_column(\"PCLASS\", F.to_varchar(F.col(\"PCLASS\"))).with_column(\"P_ID\", F.lit(1)).filter(F.col(\"EMBARKED\").is_not_null())\n",
    "input_df.select(*input_cols,  F.col(\"P_ID\") \n",
    "                , F.call_table_function(\"predict_survive_batch_udtf\", *input_cols).over(partition_by=[\"P_ID\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ab3568",
   "metadata": {},
   "source": [
    "#### Using SnowflakeFile with UDTF\n",
    "**SnowflakeFile** allows us to read files from a Snowflake stage without using the **add_import**, instead we pass the reference to a file as the input to the UDxF.\n",
    "\n",
    "A good use case is to extract data from files in formats that is not supported out of the box by Snowflake, in this example we will get data from a fixed width file.\n",
    "\n",
    "Start by creating a fixed width file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e864c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = (\n",
    "    \"id8141    360.242940   149.910199   11950.7\\n\"\n",
    "    \"id1594    444.953632   166.985655   11788.4\\n\"\n",
    "    \"id1849    364.136849   183.628767   11806.2\\n\"\n",
    "    \"id1230    413.836124   184.375703   11916.8\\n\"\n",
    "    \"id1948    502.953953   173.237159   12468.3\"\n",
    ")\n",
    "with open(f\"{data_path}fixed_width.txt\", \"w\") as f:\n",
    "    f.write(data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c75b5aa",
   "metadata": {},
   "source": [
    "Upload the file to a stage and enable directory table on it, https://docs.snowflake.com/en/user-guide/data-load-dirtables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58800c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "snf_session.file.put(f\"{data_path}fixed_width.txt\", f\"@{udf_stage_name}/fixed_width/\", auto_compress=False, overwrite=True)\n",
    "# We need to enable directory table on the stage and then refresh it so the files are visible\n",
    "snf_session.sql(\"ALTER STAGE \" + udf_stage_name + \" SET DIRECTORY = (ENABLE = TRUE)\").collect()\n",
    "snf_session.sql(f\"ALTER STAGE {udf_stage_name} REFRESH\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14203f28",
   "metadata": {},
   "source": [
    "Create a UDTF that opens the file and creates a Pandas DataFrame on it, based on colspec, and then returns it as columns and rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013c6a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.files import SnowflakeFile\n",
    "\n",
    "@F.udtf(name=\"extract_fixed_width_udtf\", output_schema=[\"col1\", \"col2\", \"col3\", \"col4\"], is_permanent=False, replace=True, session=snf_session, packages=['snowflake-snowpark-python'])\n",
    "class ExtractFixedWidth:\n",
    "    # Called for each row\n",
    "    def process(self, file_path: str, colspecs: list) -> Iterable[Tuple[str, float, float, float]]:\n",
    "        # Open the file, \n",
    "        with SnowflakeFile.open(file_path, 'rb', require_scoped_url=False) as f:\n",
    "            return_pd =  pd.read_fwf(f, colspecs=colspecs, header=None) \n",
    "        \n",
    "        yield from list(return_pd.itertuples(index=False, name=None))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c39b94d",
   "metadata": {},
   "source": [
    "Get the path to the file we uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c341776",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = snf_session.sql(f\"ls @{udf_stage_name}/fixed_width/\").collect()[0][0]\n",
    "input_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b67a857",
   "metadata": {},
   "source": [
    "Extract the rows and columns for the file, colspecs has the start and end for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32d4a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "colspecs = [(0, 6), (8, 20), (21, 33), (34, 43)]\n",
    "df_udtf = snf_session.table_function(\"extract_fixed_width_udtf\", F.lit(f\"@{input_file}\"), F.lit(colspecs))\n",
    "df_udtf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb982722-2d4c-4d57-87f2-aaae2a90cde1",
   "metadata": {},
   "source": [
    "# Using UDF in other languages\n",
    "\n",
    "A UDF can be created using Python, Java, Scala, JavaScript and SQL and can be used for any of the supported languages.\n",
    "\n",
    "Start by creating a Scala UDF using SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e0a5b9-a26f-4b2d-8e98-adb8c203246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scala_udf_sql = \"\"\"CREATE OR REPLACE TEMP FUNCTION scala_double_it(x INTEGER)\n",
    "RETURNS INTEGER\n",
    "LANGUAGE SCALA\n",
    "CALLED ON NULL INPUT\n",
    "RUNTIME_VERSION = 2.12\n",
    "HANDLER='Double.doubleIt'\n",
    "AS\n",
    "$$\n",
    "class Double {\n",
    "  def doubleIt(x : Integer): Integer = {\n",
    "    return 2*x;\n",
    "  }\n",
    "}\n",
    "$$;\n",
    "\"\"\"\n",
    "\n",
    "snf_session.sql(scala_udf_sql).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f228c2-6b38-4d1b-a9c5-c6638e497a0a",
   "metadata": {},
   "source": [
    "Create a Python UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5975b2-8e3d-41c3-bc67-09f295c33701",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(name=\"python_echo_varchar\", is_permanent=False, replace=True, session=snf_session)\n",
    "def python_echo_varchar(ds: T.PandasSeries[str]) -> T.PandasSeries[str]:\n",
    "    return ds.apply(lambda x: f\"Hello {x}, I'm a Python UDF!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa54c6d-5440-4d7c-a34d-6317dfdf5052",
   "metadata": {},
   "source": [
    "Create a DataFrame with testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce19c8fe-db8a-4194-b116-8327dbbe136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_udf_df = snf_session.create_dataframe([['Mats', 2], ['Pia', 3]], schema=[\"name\", \"value\"])\n",
    "test_udf_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36f705a-9f94-490a-8290-e9c11d4131d9",
   "metadata": {},
   "source": [
    "Use both the SCALA and PYTHON UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99702f45-85f9-4657-9dc7-b501191ce88b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_udf_df.select(F.call_function(\"scala_double_it\", F.col(\"value\")).as_(\"SCALA_UDF_RES\"), \n",
    "                   F.call_function(\"python_echo_varchar\", F.col(\"name\")).as_(\"PYTHON_UDF_RES\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa639e44-05d0-4ada-a1d7-15d5b8048136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close session will drop all temp object created\n",
    "snf_session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0fe2a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
